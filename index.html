<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Martin Jaroš" />
  <title>Augmented reality navigation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Augmented reality navigation</h1>
<h2 class="author">Martin Jaroš</h2>
<h3 class="date">Dec. 2013</h3>
</div>
<div id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#augmented-reality"><span class="toc-section-number">1</span> Augmented reality</a><ul>
<li><a href="#design-goals"><span class="toc-section-number">1.1</span> Design goals</a></li>
<li><a href="#hardware-limitations"><span class="toc-section-number">1.2</span> Hardware limitations</a></li>
</ul></li>
<li><a href="#application"><span class="toc-section-number">2</span> Application</a><ul>
<li><a href="#linux-kernel"><span class="toc-section-number">2.1</span> Linux kernel</a></li>
<li><a href="#video-subsystem"><span class="toc-section-number">2.2</span> Video subsystem</a></li>
<li><a href="#graphics-subsystem"><span class="toc-section-number">2.3</span> Graphics subsystem</a></li>
<li><a href="#inertial-measurement-subsystem"><span class="toc-section-number">2.4</span> Inertial measurement subsystem</a></li>
<li><a href="#satellite-navigation-subsystem"><span class="toc-section-number">2.5</span> Satellite navigation subsystem</a></li>
</ul></li>
<li><a href="#hardware"><span class="toc-section-number">3</span> Hardware</a></li>
<li><a href="#conclusion"><span class="toc-section-number">4</span> Conclusion</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
<h1 id="preface" class="unnumbered"><a href="#preface">Preface</a></h1>
<blockquote>
Introduction<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
</blockquote>
<p><strong>Source code for the application</strong> is available at<br /><a href="https://github.com/martinjaros/augmented-reality-navigation">https://github.com/martinjaros/augmented-reality-navigation</a></p>
<p><strong>Source code for this document</strong> is available at<br /><a href="https://github.com/martinjaros/augmented-reality-navigation/tree/thesis">https://github.com/martinjaros/augmented-reality-navigation/tree/thesis</a></p>
<p><strong>HTML version of this document</strong> is available at<br /><a href="http://martinjaros.github.io/augmented-reality-navigation">http://martinjaros.github.io/augmented-reality-navigation</a></p>
<h1 id="augmented-reality"><a href="#augmented-reality"><span class="header-section-number">1</span> Augmented reality</a></h1>
<h2 id="design-goals"><a href="#design-goals"><span class="header-section-number">1.1</span> Design goals</a></h2>
<blockquote>
Design goals and project overview<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
</blockquote>
<h2 id="hardware-limitations"><a href="#hardware-limitations"><span class="header-section-number">1.2</span> Hardware limitations</a></h2>
<p>Developing an application for an embedded device faces a basic problem, as there are big differences between these devices it is hard to support the hardware and make the application portable. In order to reuse code and reduce application size, libraries are generally used. To provide enough abstraction operating system is used. There are many kernels specially tailored for embedded applications such as <a href="http://www.freertos.org">FreeRTOS</a>, <a href="http://www.elinux.org">Linux</a> or proprietary <a href="http://www.windriver.com/products/vxworks">VxWorks</a>, <a href="http://www.microsoft.com/windowsembedded">Windows CE</a>. Linux kernel has been chosen for this project.</p>
<p><strong>Advantages of the Linux kernel</strong></p>
<ul>
<li>free and open-source, well documented</li>
<li>highly configurable and portable</li>
<li>highly standardized, POSIX compliant</li>
<li>large amount of drivers, good manufacturer support</li>
<li>great community support, many tutorials</li>
</ul>
<p><strong>Disadvantages of the Linux kernel</strong></p>
<ul>
<li>large code base</li>
<li>steep learning curve</li>
<li>high hardware requirements</li>
</ul>
<p>While the application is designed to be highly portable depending only on the kernel itself, several devices has been chosen as the reference.</p>
<p><strong><a href="http://www.ti.com/product/omap4460">OMAP4460</a> application processor</strong><sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup></p>
<ul>
<li>two ARM Cortex-A9 SMP general-purpose processors</li>
<li>IVA 3 video accelerator, 1080p capable</li>
<li>image signal processor, 20MP capable</li>
<li>SGX540 3D graphics accelerator, OpenGL ES 2.0 compatible</li>
<li>HDMI v1.3 video output</li>
</ul>
<p><strong><a href="http://www.invensense.com/mems/gyro/mpu9150.html">MPU-9150</a> motion tracking device</strong><sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup></p>
<ul>
<li>embedded MPU-6050 3-axis gyroscope and accelerometer</li>
<li>embedded AK8975 3-axis digital compass</li>
<li>fully programmable, I<sup>2</sup>C interface</li>
</ul>
<p><strong><a href="http://www.ovt.com/products/sensor.php?id=93">OV5640</a> image sensor</strong><sup><a href="#fn3" class="footnoteRef" id="fnref3">3</a></sup></p>
<ul>
<li>1080p, 5MP resolution</li>
<li>raw RGB or YUV output</li>
</ul>
<h1 id="application"><a href="#application"><span class="header-section-number">2</span> Application</a></h1>
<h2 id="linux-kernel"><a href="#linux-kernel"><span class="header-section-number">2.1</span> Linux kernel</a></h2>
<p>Programs running in Linux are divided into two groups, <em>kernel-space</em> and <em>user-space</em>. Only kernel and its runtime modules are allowed to execute in <em>kernel-space</em>, while all other programs runs as processes in <em>user-space</em>.</p>
<p><strong>kernel-space</strong></p>
<ul>
<li>real-time CPU usage</li>
<li>physical memory access</li>
</ul>
<p><strong>user-space</strong></p>
<ul>
<li>scheduled CPU usage</li>
<li>virtual memory access</li>
</ul>
<p>In Linux each process runs in a sandbox, isolated from the rest of the system. Processes access virtual memory unique to them, they cannot access memory assigned for other processes nor memory managed by the kernel. Their execution is not real-time, but they are assigned restricted processor time by the kernel. They may communicate with outside environment by several means</p>
<ul>
<li>Arguments and environment variables</li>
<li>Standard input, output and error output</li>
<li>Virtual File System</li>
<li>Signals</li>
<li>Sockets</li>
<li>Memory mapping</li>
</ul>
<p>Each process is ran with several arguments in a specific environment with three default file descriptors. For example running</p>
<p><code class="sourceCode bash"><span class="ot">VARIABLE=</span>value <span class="kw">./executable</span> argument1 argument2 <span class="kw">&lt;</span>input <span class="kw">1&gt;</span>output <span class="kw">2&gt;</span>error</code></p>
<p>will execute <em>executable</em> with environment variable <em>VARIABLE</em> of value <em>value</em> with two arguments <em>argument1</em> and <em>argument2</em>. Standard input will be read from file <em>input</em> while regular output will be written to file <em>output</em> and error output to file <em>error</em>. This process may further communicate by accessing files in the Virtual File System, kernel may expose useful process information for example via <code>procfs</code> file-system usually mounted at <code>/proc</code>. Other types of communication are signals (which may be sent between processes or by kernel) and network sockets. With internal network loop-back device, network style inter process communication is possible using standard protocols (UDP, TCP, ...). Memory mapping is a way to request access to some part of the physical memory.</p>
<p>Processes may run with numerous threads, each thread has preemptively scheduled execution. Threads share memory within a process, memory access to these shared resources must done with care to avoid race conditions and data corruption. Kernel provides <em>mutex</em> objects to lock threads and avoid simultaneous memory access. Each shared resource should be attached to a <em>mutex</em>, which is locked during access to this resource. Thread must not lock <em>mutex</em> while still holding lock to this or any other <em>mutex</em> in order to avoid dead-locking.</p>
<p><strong>Source example for using posix threads</strong></p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
</pre></td><td class="sourceCode"><pre><code class="sourceCode c"><span class="ot">#include &lt;stdio.h&gt;</span>
<span class="ot">#include &lt;pthread.h&gt;</span>

<span class="dt">void</span> *worker1(<span class="dt">void</span> *arg)
{
    pthread_mutex_t *mutex = (pthread_mutex_t*)arg;
    <span class="dt">static</span> <span class="dt">char</span> buffer[<span class="dv">64</span>];

    <span class="co">// Lock mutex to restrict access to stdin and stdout</span>
    pthread_mutex_lock(mutex);
    printf(<span class="st">&quot;This is worker 1, enter something: &quot;</span>);
    scanf(<span class="st">&quot;%64s&quot;</span>, buffer);
    pthread_mutex_unlock(mutex);

    <span class="kw">return</span> (<span class="dt">void</span>*)buffer;
}

<span class="dt">void</span> *worker2(<span class="dt">void</span> *arg)
{
    pthread_mutex_t *mutex = (pthread_mutex_t*)arg;
    <span class="dt">static</span> <span class="dt">char</span> buffer[<span class="dv">64</span>];

    <span class="co">// Lock mutex to restrict access to stdin and stdout</span>
    pthread_mutex_lock(mutex);
    printf(<span class="st">&quot;This is worker 2, enter something: &quot;</span>);
    scanf(<span class="st">&quot;%64s&quot;</span>, buffer);
    pthread_mutex_unlock(mutex);

    <span class="kw">return</span> (<span class="dt">void</span>*)buffer;
}

<span class="dt">int</span> main()
{
    pthread_mutex_t mutex;
    pthread_t thread1, thread2;
    <span class="dt">char</span> *retval1, *retval2;

    <span class="co">// Initialize two threads with shared mutex, use default parameters</span>
    pthread_mutex_init(&amp;mutex, NULL);
    pthread_create(&amp;thread1, NULL, worker1, (<span class="dt">void</span>*)&amp;mutex);
    pthread_create(&amp;thread2, NULL, worker2, (<span class="dt">void</span>*)&amp;mutex);

    <span class="co">// Wait for both threads to finish and display results</span>
    pthread_join(thread1, (<span class="dt">void</span>**)&amp;retval1);
    pthread_join(thread2, (<span class="dt">void</span>**)&amp;retval2);
    printf(<span class="st">&quot;Thread 1 returned with `%s`.</span><span class="ch">\n</span><span class="st">&quot;</span>, retval1);
    printf(<span class="st">&quot;Thread 2 returned with `%s`.</span><span class="ch">\n</span><span class="st">&quot;</span>, retval2);

    pthread_mutex_destroy(&amp;mutex);
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre></td></tr></table>
<p>Linux kernel has monolithic structure, so all device drivers resides in the kernel. From application point of view, this means that all peripheral access must be done through the standard library and Virtual File System. Individual devices are accessible as device files defined by major and minor number typically located at <code>/dev</code>. These files could be created automatically by kernel (<code>devtmpfs</code> file-system), by daemon (<a href="http://linux.die.net/man/8/udev"><code>udev(8)</code></a>)), or manually by <a href="http://linux.die.net/man/1/mknod"><code>mknod(1)</code></a>. Complete kernel device model is exported as <code>sysfs</code> file-system and typically mounted at <code>/sys</code>.</p>
<table>
<caption>Available functions for working with device file descriptors</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function name</strong></th>
<th align="left"><strong>Access type</strong></th>
<th align="left"><strong>Typical usage</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="http://linux.die.net/man/2/select"><code>select()</code></a>, <a href="http://linux.die.net/man/2/poll"><code>poll()</code></a></td>
<td align="left">event</td>
<td align="left">Synchronization, multiplexing, event handling</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linux.die.net/man/2/ioctl"><code>ioctl()</code></a></td>
<td align="left">structure</td>
<td align="left">Configuration, register access</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linux.die.net/man/2/read"><code>read()</code></a>, <a href="http://linux.die.net/man/2/write"><code>write()</code></a></td>
<td align="left">stream</td>
<td align="left">Raw data buffers, byte streams</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linux.die.net/man/2/mmap"><code>mmap()</code></a></td>
<td align="left">block</td>
<td align="left">High throughput data transfers</td>
</tr>
</tbody>
</table>
<p>For example let's assume a generic peripheral device connected by the I<sup>2</sup>C bus. First, to tell kernel there is such a device, the <code>sysfs</code> file-system may be used</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> <span class="ot">$DEVICE_NAME</span> <span class="ot">$DEVICE_ADDRESS</span> <span class="kw">&gt;</span> /sys/bus/i2c/devices/i2c-1/new_device</code></p>
<p>This should create a special file in <code>/dev</code>, which should be opened by <a href="http://linux.die.net/man/2/open"><code>open()</code></a> to get a file descriptor for this device. Device driver may export some <em>ioctl</em> requests, each request is defined by a number and a structure passed between the application and the kernel. Driver should define requests for controlling the device, maybe accessing its internal registers and configuring a data stream. Each request is called by</p>
<p><code class="sourceCode c">ioctl(fd, REQNUM, &amp;data);</code></p>
<p>where <em>fd</em> is the file descriptor, <em>REQNUM</em> is the request number defined in the driver header and <em>data</em> is the structure passed to the kernel. This request will be synchronously processed by the kernel and the result stored in the <em>data</em> structure. Let's assume this devices has been configured to stream an integer value every second to the application. To synchronize with this timing application may use</p>
<p><code class="sourceCode c"><span class="kw">struct</span> pollfd fds = {fd, POLLIN};</code><br /><code class="sourceCode c">poll(&amp;fds, <span class="dv">1</span>, -<span class="dv">1</span>);</code></p>
<p>which will block infinitely until there is a value ready to be read. To actually read it,</p>
<p><code class="sourceCode c"><span class="dt">int</span> buffer[<span class="dv">1</span>];</code><br /><code class="sourceCode c">ssize_t num = read(fd, buffer, <span class="kw">sizeof</span>(buffer));</code></p>
<p>will copy this value to the buffer. Copying causes performance issues if there are very large amounts of data. To access this data directly without copying them, application has to map physical memory used by the driver. This allows for example direct access to a DMA channel, it should be noted that this memory may still be needed by kernel, so there should be some kind of dynamic access restriction, possibly via <em>ioctl</em> requests (this would be driver specific).</p>
<h2 id="video-subsystem"><a href="#video-subsystem"><span class="header-section-number">2.2</span> Video subsystem</a></h2>
<p>Video support in Linux kernel is maintained by the LinuxTV<sup><a href="#fn4" class="footnoteRef" id="fnref4">4</a></sup> project, it implements the <em>videodev2</em> kernel module and defines the <em>V4L2</em> interface. Modules are part of the mainline kernel at <code>drivers/media/video/*</code> with header <code>linux/videodev2.h</code>. The core module is enabled by the <em>VIDEO_V4L2</em> configuration option, specific device drivers should be enabled by their respective options. <em>V4L2</em> is the latest revision and is the most widespread video interface throughout Linux, drives are available from most hardware manufactures and usually mainlined or available as patches. The Linux Media Infrastructure API<sup><a href="#fn5" class="footnoteRef" id="fnref5">5</a></sup> is a well documented interface shared by all devices. It provides abstraction layer for various device implementations, separating the platform details from the applications. Each video device has its device file and is controlled via <em>ioctl</em> calls. For streaming standard I/O functions are supported, but the memory mapping is preferred, this allows passing only pointers between the application and the kernel, instead of unnecessary copying the data around.</p>
<table>
<caption>ioctl calls defined in <code>linux/videodev2.h</code></caption>
<thead>
<tr class="header">
<th align="left"><strong>Name</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-querycap.html">VIDIOC_QUERYCAP</a></td>
<td align="left">Query device capabilities</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-g-fmt.html">VIDIOC_G_FMT</a></td>
<td align="left">Get the data format</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-g-fmt.html">VIDIOC_S_FMT</a></td>
<td align="left">Set the data format</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-reqbufs.html">VIDIOC_REQBUFS</a></td>
<td align="left">Initiate memory mapping</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-querybuf.html">VIDIOC_QUERYBUF</a></td>
<td align="left">Query the status of a buffer</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-qbuf.html">VIDIOC_QBUF</a></td>
<td align="left">Enqueue buffer to the kernel</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-qbuf.html">VIDEOC_DQBUF</a></td>
<td align="left">Dequeue buffer from the kernel</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-streamon.html">VIDIOC_STREAMON</a></td>
<td align="left">Start streaming</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-streamon.html">VIDIOC_STREAMOFF</a></td>
<td align="left">Stop streaming</td>
</tr>
</tbody>
</table>
<p>Application sets the format first, then requests and maps buffers from the kernel. Buffers are exchanged between the kernel and the application. When the buffer is enqueued, it will be available for the kernel to capture data to it. When the buffer is dequeued, kernel will not access the buffer and application may read the data. After all buffer are enqueued application starts the stream. Polling is used to wait for the kernel until it fills the buffer, buffer should not be accessed simultaneously by the kernel and the application. After processing the buffer, application should return it back to the kernel queue. Note that buffers should be properly unmapped by the application after stopping the stream.</p>
<div class="figure">
<img src="images/v4l2capture.svg" alt="V4L2 capture" /><p class="caption">V4L2 capture</p>
</div>
<p><strong>Source example for simple video capture</strong></p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
</pre></td><td class="sourceCode"><pre><code class="sourceCode c"><span class="ot">#include &lt;fcntl.h&gt;</span>
<span class="ot">#include &lt;unistd.h&gt;</span>
<span class="ot">#include &lt;poll.h&gt;</span>
<span class="ot">#include &lt;sys/mman.h&gt;</span>
<span class="ot">#include &lt;sys/ioctl.h&gt;</span>
<span class="ot">#include &lt;linux/videodev2.h&gt;</span>

<span class="dt">int</span> main()
{
    <span class="co">// Open device</span>
    <span class="dt">int</span> fd = open(<span class="st">&quot;/dev/video0&quot;</span>, O_RDWR | O_NONBLOCK);

    <span class="co">// Set video format</span>
    <span class="kw">struct</span> v4l2_format format =
    {
        .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
        .fmt =
        {
            .pix =
            {
                .width = <span class="dv">320</span>,
                .height = <span class="dv">240</span>,
                .pixelformat = V4L2_PIX_FMT_RGB32,
                .field = V4L2_FIELD_NONE,
                .colorspace = V4L2_COLORSPACE_SMPTE170M,
            },
        },
    };
    ioctl(fd, VIDIOC_S_FMT, &amp;format);

    <span class="co">// Request buffers</span>
    <span class="kw">struct</span> v4l2_requestbuffers requestbuffers =
    {
        .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
        .memory = V4L2_MEMORY_MMAP,
        .count = <span class="dv">4</span>,
    };
    ioctl(fd, VIDIOC_REQBUFS, &amp;requestbuffers);
    <span class="dt">void</span> *pbuffers[requestbuffers.count];

    <span class="co">// Map and enqueue buffers</span>
    <span class="dt">int</span> i;
    <span class="kw">for</span>(i = <span class="dv">0</span>; i &lt; requestbuffers.count; i++)
    {
        <span class="kw">struct</span> v4l2_buffer buffer = 
        {
            .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
            .memory = V4L2_MEMORY_MMAP,
            .index = i,
        };
        ioctl(fd, VIDIOC_QUERYBUF, &amp;buffer);
        pbuffers[i] = mmap(NULL, buffer.length,
                           PROT_READ | PROT_WRITE, MAP_SHARED,
                           fd, buffer.m.offset);
        ioctl(fd, VIDIOC_QBUF, &amp;buffer);
    }

    <span class="co">// Start stream</span>
    <span class="kw">enum</span> v4l2_buf_type buf_type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    ioctl(fd, VIDIOC_STREAMON, &amp;buf_type);

    <span class="kw">while</span>(<span class="dv">1</span>)
    {
        <span class="co">// Synchronize</span>
        <span class="kw">struct</span> pollfd fds = 
        {
            .fd = fd,
            .events = POLLIN
        };
        poll(&amp;fds, <span class="dv">1</span>, -<span class="dv">1</span>);

        <span class="co">// Dump buffer to stdout</span>
        <span class="kw">struct</span> v4l2_buffer buffer = 
        {
            .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
            .memory = V4L2_MEMORY_MMAP,
        };
        ioctl(fd, VIDIOC_DQBUF, &amp;buffer);
        write(<span class="dv">1</span>, pbuffers[buffer.index], buffer.bytesused);
        ioctl(fd, VIDIOC_QBUF, &amp;buffer);
    }
}</code></pre></td></tr></table>
<!-- -->

<p>The image format is specified using the little-endian four-character code (FOURCC). V4L2 defines<sup><a href="#fn6" class="footnoteRef" id="fnref6">6</a></sup> several formats and provides <code>v4l2_fourcc()</code> macro to create a format code from four characters. As described later in the <a href="#graphics-subsystem">graphics subsystem</a> chapter, graphics uses natively the <em>RGB4</em> format. This format is defined as a single plane with one sample per pixel and four bytes per sample. These bytes represents red, green and blue channel values respectively. Image size is therefore <span class="math">\(width \cdot height \cdot 4\)</span> bytes. Many image sensors however support <em>YUV</em> color-space, for example the <em>YU12</em> format. This one is defined as three planes, the first plane with one luminance sample per pixel and the second and third plane with one chroma sample per four pixels (2 pixels per row, interleaved). Each sample has one byte, this format is also referenced as <em>YUV 4:2:0</em> and its image size is <span class="math">\(width \cdot height \cdot 1.5\)</span> bytes. The luminance and chroma of a pixel is defined as</p>
<ol style="list-style-type: example">
<li><p><span class="math">\(E_Y = W_R \cdot E_R + (1-W_R-W_B) \cdot E_G + W_B \cdot E_B\)</span></p></li>
<li><p><span class="math">\(E_{C_r} = \frac {0.5 (E_R - E_Y)} {1 - W_R}\)</span></p></li>
<li><p><span class="math">\(E_{C_b} = \frac {0.5 (E_B - E_Y)} {1 - W_B}\)</span></p></li>
</ol>
<p>where E<sub>R</sub>, E<sub>G</sub>, E<sub>B</sub> are normalized color values and W<sub>R</sub>, W<sub>B</sub> are their weights. ITU-R Rec. BT.601<sup><a href="#fn7" class="footnoteRef" id="fnref7">7</a></sup> defines weights as 0.299 and 0.114 respectively, it also defines how they are quantized</p>
<ol start="4" style="list-style-type: example">
<li><p><span class="math">\(Y = 219 E_Y + 16\)</span></p></li>
<li><p><span class="math">\(C_r = 224 E_{C_r} + 128\)</span></p></li>
<li><p><span class="math">\(C_b = 224 E_{C_b} + 128\)</span></p></li>
</ol>
<p>To calculate R, G, B values from Y, Cr, Cb values, inverse formulas must be used</p>
<ol start="7" style="list-style-type: example">
<li><p><span class="math">\(E_Y = \frac {Y - 16} {219}\)</span></p></li>
<li><p><span class="math">\(E_{C_r} =  \frac {C_r - 128} {224}\)</span></p></li>
<li><p><span class="math">\(E_{C_b} = \frac {C_b - 128} {224}\)</span></p></li>
<li><p><span class="math">\(E_R = E_Y + 2 E_{C_r} (1 - W_R)\)</span></p></li>
<li><p><span class="math">\(E_G = E_Y - 2 E_{C_r} \frac {W_R - {W_R}^2} {W_G} - 2 E_{C_b} \frac {W_B - {W_B}^2} {W_G}\)</span></p></li>
<li><p><span class="math">\(E_B = E_Y + 2 E_{C_b} (1 - W_B)\)</span></p></li>
</ol>
<p>It should be noted that not all devices may use the BT.601 recommendation, V4L2 refers to it as <em>V4L2_COLORSPACE_SMPTE170M</em> in the <em>VIDIOC_S_FMT</em> request structure.</p>
<p><strong>GLSL implementation of the YUV to RGB conversion</strong> (see <a href="#graphics-subsystem">graphics subsystem</a> chapter for description of GLSL)</p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="sourceCode"><pre><code class="sourceCode c">uniform sampler2D texY, texU, texV;
varying vec2 texCoord;

<span class="dt">void</span> main()
{
    <span class="dt">float</span> y = texture2D(texY, texCoord).a * <span class="fl">1.1644</span> - <span class="fl">0.062745</span>;
    <span class="dt">float</span> u = texture2D(texU, texCoord / <span class="dv">2</span>).a - <span class="fl">0.5</span>;
    <span class="dt">float</span> v = texture2D(texV, texCoord / <span class="dv">2</span>).a - <span class="fl">0.5</span>;

    gl_FragColor = vec4(
        y + <span class="fl">1.596</span> * v,
        y - <span class="fl">0.39176</span> * v - <span class="fl">0.81297</span> * u,
        y + <span class="fl">2.0172</span> * u,
        <span class="fl">1.0</span>);
}</code></pre></td></tr></table>
<p>There is a module <em>v4l2loopback</em><sup><a href="#fn8" class="footnoteRef" id="fnref8">8</a></sup> which creates a video loop-back device, similar to network loop-back, allowing piping two video applications together. This is very useful not only for testing, but also for implementation of intermediate decoders. GStreamer<sup><a href="#fn9" class="footnoteRef" id="fnref9">9</a></sup> is a powerful multimedia framework widespread in Linux distributions, composed of a core infrastructure and hundreds of plug-ins. This command will create synthetic <em>RGB4</em> video stream for the application, useful for testing</p>
<p><code class="sourceCode bash"><span class="kw">modprobe</span> v4l2loopback</code><br /><code class="sourceCode bash"><span class="kw">gst-launch</span> videotestsrc pattern=solid-color foreground-color=0xE0F0E0 ! \</code><br /><code class="sourceCode bash"><span class="st">&quot;video/x-raw,format=RGBx,width=800,height=600,framerate=20/1&quot;</span> <span class="kw">\</span></code><br /><code>! v4l2sink device=/dev/video0</code></p>
<p>Texas Instruments distributes a meta package<sup><a href="#fn10" class="footnoteRef" id="fnref10">10</a></sup> for their OMAP platform featuring all required modules and DSP firmware. This includes kernel modules for <em>SysLink</em> inter-chip communication library, <em>Distributed Codec Engine</em> library and <em>ducati</em> plug-in for GStreamer. With the meta-package installed, it is very easy and efficient to implement mainstream encoded video formats. For example following command will create GStreamer pipeline to receive video payload over a network socket from an IP camera, decode it and push it to the loop-back device for the application. MPEG-4 AVC (H.264) decoder of the IVA 3 is used in this example.</p>
<p><code class="sourceCode bash"><span class="kw">modprobe</span> v4l2loopback</code><br /><code class="sourceCode bash"><span class="kw">gst-launch</span> udpsrc port=5004 caps=\</code><br /><code class="sourceCode bash"><span class="st">&quot;application/x-rtp,media=video,payload=96,clock-rate=90000,encoding-name=H264&quot;</span> <span class="kw">\</span></code><br /><code>! rtph264depay ! h264parse ! ducatih264dec ! v4l2sink device=/dev/video0</code></p>
<p>On OMAP4460 this would consume only about 15% of the CPU time as the decoding is done by the IVA 3 video accelerator in parallel to the CPU which only passes pointers around and handles synchronization. Output format is <em>NV12</em> which is similar to <em>YU12</em> format described earlier, but there is only one chroma plane with two-byte samples, first byte being the U channel and the second byte the V channel, sampling is same 4:2:0. The YUV to RGB color space conversion must take place here, preferably implemented on the GPU as described above.</p>
<p>Cortex-A9 cores on the OMAP4460 also have the NEON co-processor, capable of vector floating point math. Although not very supported by the GCC C compiler, there are many assembly written libraries implementing coders with the NEON acceleration. For example the <em>libjpeg-turbo</em><sup><a href="#fn11" class="footnoteRef" id="fnref11">11</a></sup> library is implementing the <em>libjpeg</em> interface. It is useful for USB cameras, as the USB throughput is not high enough for raw high definition video, but is sufficient with JPEG coding (as most USB cameras supports JPEG, but does not support H.264). 1080p JPEG stream decoded with this library via its GStreamer plug-in will consume about 90% of the single CPU core time (note that there are two CPU cores available). However, comparable to the AVC, JPEG encoding will cause visible quality degradation in the raw stream (video looks grainy).</p>
<h2 id="graphics-subsystem"><a href="#graphics-subsystem"><span class="header-section-number">2.3</span> Graphics subsystem</a></h2>
<blockquote>
Graphics stack, OpenGL ES 2.0<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
</blockquote>
<h2 id="inertial-measurement-subsystem"><a href="#inertial-measurement-subsystem"><span class="header-section-number">2.4</span> Inertial measurement subsystem</a></h2>
<blockquote>
Industrial I/O module and drivers, DCM algorithm<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
</blockquote>
<h2 id="satellite-navigation-subsystem"><a href="#satellite-navigation-subsystem"><span class="header-section-number">2.5</span> Satellite navigation subsystem</a></h2>
<blockquote>
TTY module, stty, socat, GPS, NMEA 0183<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
</blockquote>
<h1 id="hardware"><a href="#hardware"><span class="header-section-number">3</span> Hardware</a></h1>
<blockquote>
Existing modules, designs<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
</blockquote>
<h1 id="conclusion"><a href="#conclusion"><span class="header-section-number">4</span> Conclusion</a></h1>
<blockquote>
Conclusion<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
</blockquote>
<h1 id="references" class="unnumbered"><a href="#references">References</a></h1>
<p>[1] BIMBER, O.; RASKAR, R. Spatial augmented reality: merging real and virtual worlds. Wellesley: A K Peters, 2005, 369 p. ISBN 15-688-1230-2.</p>
<p>[2] JAZAR, Reza N. Theory of applied robotics: kinematics, dynamics, and control. 2nd ed. New York: Springer, 2010, 883 p. ISBN 978-1-4419-1749-2.</p>
<p>[3] KENNEDY, Melita. Understanding map projections. Redlands: ESRI, 2000, 110 p. ISBN 15-894-8003-1.</p>
<p>[4] Texas Instruments. OMAP 4460 Multimedia Device [online]. 2012 - [cit. 8. Nov 2012]. Available: <a href="http://www.ti.com/product/omap4460">http://www.ti.com/product/omap4460</a>.</p>
<p>[5] InvenSense. MPU-9150 Nine-Axis MEMS MotionTracking™ Device [online]. 2013 - [cit. 10. Nov 2013]. Available: <a href="http://www.invensense.com/mems/gyro/mpu9150.html">http://www.invensense.com/mems/gyro/mpu9150.html</a>.</p>
<p>[6] Consultative Committee on International Radio. ITU-R Recommendation BT.601 [online]. 2011 - [cit. 10. Nov 2013]. Available: <a href="http://www.itu.int/rec/R-REC-BT.601/en">http://www.itu.int/rec/R-REC-BT.601/en</a>.</p>
<p>[7] Khronos Group. OpenGL ES 2.X - for Programmable Hardware [online]. 2013 - [cit. 10. Nov 2013]. Available: <a href="http://www.khronos.org/opengles/2_X/">http://www.khronos.org/opengles/2_X/</a>.</p>
<p>[8] National Marine Electronics Association. NMEA 0183 [online]. 2008 - [cit. 10. Nov 2013]. Available: <a href="http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp">http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp</a>.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>OMAP4460 Technical reference manual<br /><a href="http://www.ti.com/litv/pdf/swpu235aa">http://www.ti.com/litv/pdf/swpu235aa</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>MPU-9150 Product specification<br /><a href="http://invensense.com/mems/gyro/documents/PS-MPU-9150A-00v4_3.pdf">http://invensense.com/mems/gyro/documents/PS-MPU-9150A-00v4_3.pdf</a><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>OV5640 Product brief<br /><a href="http://www.ovt.com/download_document.php?type=sensor&amp;sensorid=93">http://www.ovt.com/download_document.php?type=sensor&amp;sensorid=93</a><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>LinuxTV project<br /><a href="http://linuxtv.org/">http://linuxtv.org/</a><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Linux Media Infrastructure API<br /><a href="http://linuxtv.org/downloads/v4l-dvb-apis/">http://linuxtv.org/downloads/v4l-dvb-apis/</a><a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>V4L2 image format definitions<br /><a href="http://linuxtv.org/downloads/v4l-dvb-apis/pixfmt.html">http://linuxtv.org/downloads/v4l-dvb-apis/pixfmt.html</a><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>ITU-R Recommendation BT.601-7<br /><a href="http://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.601-7-201103-I!!PDF-E.pdf">http://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.601-7-201103-I!!PDF-E.pdf</a><a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>V4L2 loopback device module<br /><a href="https://github.com/umlaeute/v4l2loopback">https://github.com/umlaeute/v4l2loopback</a><a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>GStreamer home page<br /><a href="http://gstreamer.freedesktop.org/">http://gstreamer.freedesktop.org/</a><a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>TI OMAP trunk PPA<br /><a href="https://launchpad.net/~tiomap-dev/+archive/omap-trunk">https://launchpad.net/~tiomap-dev/+archive/omap-trunk</a><a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>libjpeg-turbo library<br /><a href="http://www.libjpeg-turbo.org/">http://www.libjpeg-turbo.org/</a><a href="#fnref11">↩</a></p></li>
</ol>
</div>
</body>
</html>
