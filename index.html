<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Martin Jaroš" />
  <title>Augmented reality navigation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Augmented reality navigation</h1>
<h2 class="author">Martin Jaroš</h2>
<h3 class="date">Dec. 2013</h3>
</div>
<div id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#augmented-reality">Augmented reality</a><ul>
<li><a href="#project-overview">Project overview</a></li>
<li><a href="#hardware-limitations">Hardware limitations</a></li>
</ul></li>
<li><a href="#application">Application</a><ul>
<li><a href="#linux-kernel">Linux kernel</a></li>
<li><a href="#video-subsystem">Video subsystem</a><ul>
<li><a href="#implementation">Implementation</a></li>
</ul></li>
<li><a href="#graphics-subsystem">Graphics subsystem</a><ul>
<li><a href="#opengl-es-2.0-api">OpenGL ES 2.0 API</a></li>
<li><a href="#opengl-es-shading-language">OpenGL ES Shading Language</a></li>
<li><a href="#truetype-font-rendering">TrueType font rendering</a></li>
<li><a href="#texture-streaming-extensions">Texture streaming extensions</a></li>
<li><a href="#system-integration">System integration</a></li>
<li><a href="#implementation-1">Implementation</a></li>
</ul></li>
<li><a href="#inertial-measurement-subsystem">Inertial measurement subsystem</a><ul>
<li><a href="#industrial-io-module">Industrial I/O module</a></li>
<li><a href="#dcm-algorithm">DCM algorithm</a></li>
<li><a href="#implementation-2">Implementation</a></li>
</ul></li>
<li><a href="#satellite-navigation-subsystem">Satellite navigation subsystem</a><ul>
<li><a href="#communication">Communication</a></li>
<li><a href="#navigation">Navigation</a></li>
<li><a href="#elevation-mapping">Elevation mapping</a></li>
</ul></li>
<li><a href="#output-creation">Output creation</a></li>
</ul></li>
<li><a href="#hardware">Hardware</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#threads-example">Threads example</a></li>
<li><a href="#video-capture-example">Video capture example</a></li>
<li><a href="#colorspace-conversion-example">Colorspace conversion example</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>
<h1 id="preface" class="unnumbered"><a href="#preface">Preface</a></h1>
<p>Augmented reality technologies have been around for a while <span class="citation">[1]</span>, however their implementation in embedded systems were not possible until recently. Navigation applications require a broad spectrum of functionality such as video processing, accelerated graphical rendering and of course the navigation itself. With public access to the global satellite navigation systems such as GPS or GLONASS, precise position determination is possible worldwide. Advances in sensor technologies offer many small and effective devices, such as gyroscopes, compasses, accelerometers or barometers. The augmented reality navigation provides the future of spatial navigation systems, delivering all necessary information to the user in the most clear way by projecting it directly to the visible world. Users do not have to search for the information in specialized devices or multifunction displays, they will just see it floating around, related to where they look. Applications include automotive, aeronautical or personal navigation.</p>
<p>This work provides foundation on all aspects of designing such system. The first chapter specifies requirements and limitations for the project. The second chapter deals with the application design, it is divided per each individual subsystem. The third chapter specifies hardware details and platform realization.</p>
<h1 id="augmented-reality"><a href="#augmented-reality">Augmented reality</a></h1>
<h2 id="project-overview"><a href="#project-overview">Project overview</a></h2>
<p>Main goal of this project is to develop a device capable of rendering a real time overlay over the captured video frame. There are three external inputs, the image sensor capable of video capture feeds real-time images to the system. The GPS receiver delivers positional information and inertial sensors supplement it with spatial orientation. Expected setup is that the image and inertial sensors are on a single rack able to freely rotate around, while the GPS receiver is static, relative to the whole moving platform (vehicle for example). The overlay consists of fixed kinematic data such as speed or altitude, reference indicators such a horizon line and dynamic location markers. These will be spatially aligned with real locations visible in the video thus providing navigation information. They work in a way that wherever the camera is pointed to, specific landmarks will label currently visible locations. Complex external navigation system may be also connected. This allows integration with already existing systems, such as moving map systems, PDAs or other specialized hardware. These provides user with classic route navigation and map projection, while this project gives spatial extension to further improve total situational awareness. The analogy are the head up and head down displays, each delivering specific set of information. This project focuses on visual enhancement instead of a full featured navigation device. Overview on of the project design is in the following figure.</p>
<div class="figure">
<img src="images/overview.svg" alt="Project overview" /><p class="caption">Project overview</p>
</div>
<h2 id="hardware-limitations"><a href="#hardware-limitations">Hardware limitations</a></h2>
<p>Application is designed to run in embedded environments, where power management is very important. While many platforms features multiple symmetrical processor cores - CPUs, application should focus on lowest per-core usage as possible. This can be done by delegating specific tasks to specialized hardware. CPU is specialized in execution of single thread, integer and bitwise operations, with many branches in its code. With vector and floating point extensions they are also very efficient in computation of difficult mathematical algorithms. However they do not perform well in simple calculations over large amounts of data, where mass parallelization is possible. This is the case in graphics where special graphics processors - GPUs have been deployed. GPU consists of high number (hundreds) of simple cores, which are able to perform operations over large blocks of data. They scale efficiently with the amount of data needed to be processed due to parallelization, however they have problems with nonlinear code with branches. While CPUs have long pipelines for branch optimizations, GPUs cannot employ those, any branch in their code will be extremely inefficient and should be avoided. <a href="#graphics-subsystem">Chapter 2.3</a> focuses on this area. There are also available specialized subsystems designed and optimized for a single purpose. For example video accelerators, capable of video encoding and decoding, image capture systems or peripheral drivers. They will be mentioned in specific chapters.</p>
<p>Developing an application for an embedded device faces a problem, as there are big differences between these devices it is hard to support the hardware and make the application portable. In order to reuse code and reduce application size, libraries are generally used to create an intermediary layer between application and the hardware. However, to provide enough abstraction some sort of operating system has to be used. Operating systems may be real-time, giving applications full control, behaving just like large libraries. This is favorable approach in embedded systems as it allows precise timings. There are many such systems specially tailored for embedded applications like <a href="http://www.freertos.org">FreeRTOS</a> <span class="citation">[2]</span> or proprietary <a href="http://www.windriver.com/products/vxworks">VxWorks</a> <span class="citation">[3]</span>. On the other hand, as recent processors improved greatly in power, efficiency and capabilities, it is possible and quite feasible to run a full featured system like <a href="http://www.elinux.org">Linux</a> or proprietary <a href="http://www.microsoft.com/windowsembedded">Windows CE</a> <span class="citation">[4]</span>. Linux kernel is highly portable and configurable, although it does restrict applications from real-time use (Linux RT patches also exist for real-time applications), as all hardware dependent modules which requires full control over the hardware are part of the kernel itself, application does not need to run in real-time at all. Other advantages are free, open and well documented sources, highly standardized and POSIX compliant application interface, large amount of drivers with good manufacturer support. While its disadvantages are very large code base and steep learning curve, which may slow the initial development. Nevertheless Linux kernel has been chosen for the project, more details about its interfaces are in the <a href="#linux-kernel">chapter 2.1</a>. While the application is designed to be highly portable depending only on the kernel itself, several devices has been chosen as the reference, they are listed in the <a href="#hardware">chapter 3</a>.</p>
<h1 id="application"><a href="#application">Application</a></h1>
<p>Application is divided into four subsystems, each being standalone component. The <a href="#video-subsystem">video subsystem (chapter 2.2)</a> is responsible for enumeration and control of the video architecture and its devices. It provides the application with raw video buffers and means to configure its format. It is designed to support high range of devices from embedded image sensors to external cameras, while using single application interface and common image format. Depending on the hardware, high definition video output is expected. Video subsystem is optimized for synchronous operation with the <a href="#graphics-subsystem">graphics subsystem (chapter 2.3)</a>. Graphics subsystem utilizes platform interfaces for its graphic accelerator units to provide optimized video processing and rendering. It is hardware independent through common library support to run on most embedded systems. Its goal is to provide application with efficient methods for rendering primitives, video frames and vector fonts with object oriented interface. These methods combined will create the scene overlay over the source video in real time. Graphic output should be high definition digital, maintaining source quality. Data needed for the overlay creation are provided by <a href="#satellite-navigation-subsystem">satellite (chapter 2.5)</a> and <a href="#inertial-measurement-subsystem">inertial (chapter 2.4)</a> subsystems. They are both designed for asynchronous operation. The satellite navigation provides application with positional and kinematic data. It is responsible for communication with external navigation systems such as GPS receivers and all needed calculations. Its interfaces allows application to access required information asynchronously as needed by the rendering loop. The inertial measurement subsystem utilizes sensors needed for spatial orientation not provided by satellite navigation. As there are many such sensors, common interface is provided. Subsystem handles all initialization, control, data acquisition and required calculations. Its internal state machine provides application the requested data on demand. Application is designed to be modular and highly configurable. All constants used throughout the implementation are defined with a default value and modifiable through the configuration file. This includes for example video setup, device selection or rendering parameters.</p>
<p></p>
<h2 id="linux-kernel"><a href="#linux-kernel">Linux kernel</a></h2>
<p>Programs running in Linux are divided into two groups, kernel-space and user-space. Only kernel and its runtime modules are allowed to execute in kernel-space, they have physical memory access and use CPU in real-time. All other programs runs as processes in user-space, they have virtual memory access, which means their memory addresses are translated to the physical addresses in the background. In Linux each process runs in a sandbox, isolated from the rest of the system. Processes access memory unique to them, they cannot access memory assigned for other processes nor memory managed by the kernel. They may communicate with the outside environment by several means:</p>
<ul>
<li>Arguments and environment variables</li>
<li>Standard input, output and error output</li>
<li>Virtual File System</li>
<li>Signals</li>
<li>Sockets</li>
<li>Memory mapping</li>
</ul>
<p>Each process is ran with several arguments in a specific environment with three default file descriptors. For example running</p>
<p><code class="sourceCode bash"><span class="ot">VARIABLE=</span>value <span class="kw">./executable</span> argument1 argument2 <span class="kw">&lt;</span>input <span class="kw">1&gt;</span>output <span class="kw">2&gt;</span>error</code></p>
<p>will execute <code>executable</code> with environment variable <code>VARIABLE</code> of value <code>value</code> with two arguments <code>argument1</code> and <code>argument2</code>. Standard input will be read from file <code>input</code> while regular output will be written to file <code>output</code> and error output to file <code>error</code>. This process may further communicate by accessing files in the Virtual File System, kernel may expose useful process information for example via <code>procfs</code> file-system usually mounted at <code>/proc</code>. Other types of communication are signals (which may be sent between processes or by kernel) and network sockets. With internal network loop-back device, network style inter process communication is possible using standard protocols (UDP, TCP, ...). Memory mapping is a way to request access to some part of the physical memory.</p>
<p>Process execution is not real-time, but they are assigned restricted processor time by the kernel. They may run in numerous threads, each thread has preemptively scheduled execution. Threads share memory within a process, memory access to these shared resources must done with care to avoid race conditions and data corruption. Kernel provides <em>mutex</em> objects to lock threads and avoid simultaneous memory access. Each shared resource should be attached to a <em>mutex</em>, which is locked during access to this resource. Thread must not lock <em>mutex</em> while still holding lock to this or any other <em>mutex</em> in order to avoid dead-locking. Source code on how to use threads is in the <a href="#threads-example">threads example</a> appendix.</p>
<p>Linux kernel has monolithic structure, so all device drivers resides in the kernel-space. From application point of view, this means that all peripheral access must be done through the standard library and Virtual File System. Individual devices are accessible as device files defined by major and minor number typically located at <code>/dev</code>. These files could be created automatically by kernel (<code>devtmpfs</code> file-system), by daemon (<a href="http://linux.die.net/man/8/udev"><code>udev(8)</code></a>)), or manually by <a href="http://linux.die.net/man/1/mknod"><code>mknod(1)</code></a>. Complete kernel device model is exported as <code>sysfs</code> file-system and typically mounted at <code>/sys</code>.</p>
<table>
<caption>Available functions for working with device file descriptors</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function name</strong></th>
<th align="left"><strong>Access type</strong></th>
<th align="left"><strong>Typical usage</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="http://linux.die.net/man/2/select"><code>select()</code></a>, <a href="http://linux.die.net/man/2/poll"><code>poll()</code></a></td>
<td align="left">event</td>
<td align="left">Synchronization, multiplexing, event handling</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linux.die.net/man/2/ioctl"><code>ioctl()</code></a></td>
<td align="left">structure</td>
<td align="left">Configuration, register access</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linux.die.net/man/2/read"><code>read()</code></a>, <a href="http://linux.die.net/man/2/write"><code>write()</code></a></td>
<td align="left">stream</td>
<td align="left">Raw data buffers, byte streams</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linux.die.net/man/2/mmap"><code>mmap()</code></a></td>
<td align="left">block</td>
<td align="left">High throughput data transfers</td>
</tr>
</tbody>
</table>
<p>For example, assume a generic peripheral device connected by the I<sup>2</sup>C bus. First, to tell kernel there is such a device, the <code>sysfs</code> file-system may be used</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> <span class="ot">$DEVICE_NAME</span> <span class="ot">$DEVICE_ADDRESS</span> <span class="kw">&gt;</span> /sys/bus/i2c/devices/i2c-1/new_device</code></p>
<p>This should create a special file in <code>/dev</code>, which should be opened by <a href="http://linux.die.net/man/2/open"><code>open()</code></a> to get a file descriptor for this device. Device driver may export some <em>ioctl</em> requests, each request is defined by a number and a structure passed between the application and the kernel. Driver should define requests for controlling the device, maybe accessing its internal registers and configuring a data stream. Each request is called by</p>
<p><code class="sourceCode c">ioctl(fd, REQNUM, &amp;data);</code></p>
<p>where <code>fd</code> is the file descriptor, <code>REQNUM</code> is the request number defined in the driver header and <code>data</code> is the structure passed to the kernel. This request will be synchronously processed by the kernel and the result stored in the <code>data</code> structure. Assume this devices has been configured to stream an integer value every second to the application. To synchronize with this timing application may use</p>
<p><code class="sourceCode c"><span class="kw">struct</span> pollfd fds = {fd, POLLIN};</code><br /><code class="sourceCode c">poll(&amp;fds, <span class="dv">1</span>, -<span class="dv">1</span>);</code></p>
<p>which will block infinitely until there is a value ready to be read. To actually read it,</p>
<p><code class="sourceCode c"><span class="dt">int</span> buffer[<span class="dv">1</span>];</code><br /><code class="sourceCode c">ssize_t num = read(fd, buffer, <span class="kw">sizeof</span>(buffer));</code></p>
<p>will copy this value to the buffer. Copying causes performance issues if there are very large amounts of data. To access this data directly without copying them, application has to map physical memory used by the driver. This allows for example direct access to a DMA channel, it should be noted that this memory may still be needed by kernel, so there should be some kind of dynamic access restriction, possibly via <em>ioctl</em> requests (this would be driver specific).</p>
<h2 id="video-subsystem"><a href="#video-subsystem">Video subsystem</a></h2>
<p>Video support in Linux kernel is maintained by the <a href="http://linuxtv.org">LinuxTV</a> project, it implements the <code>videodev2</code> kernel module and defines the <em>V4L2</em> interface. Modules are part of the mainline kernel at <code>drivers/media/video/*</code> with header <code>linux/videodev2.h</code>. The core module is enabled by the <code>VIDEO_V4L2</code> configuration option, specific device drivers should be enabled by their respective options. V4L2 is the latest revision and is the most widespread video interface throughout Linux, drives are available from most hardware manufactures and usually mainlined or available as patches. The <a href="http://linuxtv.org/downloads/v4l-dvb-apis">Linux Media Infrastructure API</a> <span class="citation">[5]</span> is a well documented interface shared by all devices. It provides abstraction layer for various device implementations, separating the platform details from the applications. Individual devices are implemented in their specific drivers. These usually exports some configuration options, controllable with the <code>uvcdynctrl</code> utility tool. To list available video devices use</p>
<p><code class="sourceCode bash"><span class="kw">uvcdynctrl</span> -l</code></p>
<p>To list available frame formats supported by the device use</p>
<p><code class="sourceCode bash"><span class="kw">uvcdynctrl</span> -d DEVICE_NAME -fv</code></p>
<p>where <code>DEVICE_NAME</code> is the name returned by previous command. To list available controls exported by the driver use</p>
<p><code class="sourceCode bash"><span class="kw">uvcdynctrl</span> -d DEVICE_NAME -cv</code></p>
<p>To read value of the specific control use</p>
<p><code class="sourceCode bash"><span class="kw">uvcdynctrl</span> -d DEVICE_NAME -g <span class="st">&quot;CONTROL_NAME&quot;</span></code></p>
<p>To change the value of the control use</p>
<p><code class="sourceCode bash"><span class="kw">uvcdynctrl</span> -d DEVICE_NAME -s <span class="st">&quot;CONTROL_NAME&quot;</span> -- VALUE</code></p>
<p>Typical list of controls include</p>
<ul>
<li><code>&quot;Brightness&quot;</code></li>
<li><code>&quot;Contrast&quot;</code></li>
<li><code>&quot;Saturation&quot;</code></li>
<li><code>&quot;Hue&quot;</code></li>
<li><code>&quot;Gamma&quot;</code></li>
<li><code>&quot;White Balance Temperature, Auto&quot;</code></li>
<li><code>&quot;White Balance Temperature&quot;</code></li>
<li><code>&quot;Backlight Compensation&quot;</code></li>
<li><code>&quot;Sharpness&quot;</code></li>
<li><code>&quot;Exposure, Auto&quot;</code></li>
<li><code>&quot;Exposure (Absolute)&quot;</code></li>
<li><code>&quot;Focus, Auto&quot;</code></li>
<li><code>&quot;Focus (absolute)&quot;</code></li>
</ul>
<p>Auto focus usually gives poor quality for outdoor usage so infinite absolute focus should be set. The sharpness refers to a proprietary image enhancement algorithms which may sometimes give over-enhanced feeling of the image.</p>
<p>Each video device has its device file and is controlled via <em>ioctl</em> calls. For streaming, standard I/O functions are supported, but the memory mapping is preferred, this allows passing only pointers between the application and the kernel, instead of unnecessary copying the data around. Available <em>ioctl</em> calls are:</p>
<table>
<caption>V4L2 ioctl calls defined in <code>linux/videodev2.h</code></caption>
<thead>
<tr class="header">
<th align="left"><strong>Name</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-querycap.html">VIDIOC_QUERYCAP</a></td>
<td align="left">Query device capabilities</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-g-fmt.html">VIDIOC_G_FMT</a></td>
<td align="left">Get the data format</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-g-fmt.html">VIDIOC_S_FMT</a></td>
<td align="left">Set the data format</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-reqbufs.html">VIDIOC_REQBUFS</a></td>
<td align="left">Initiate memory mapping</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-querybuf.html">VIDIOC_QUERYBUF</a></td>
<td align="left">Query the status of a buffer</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-qbuf.html">VIDIOC_QBUF</a></td>
<td align="left">Enqueue buffer to the kernel</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-qbuf.html">VIDEOC_DQBUF</a></td>
<td align="left">Dequeue buffer from the kernel</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-streamon.html">VIDIOC_STREAMON</a></td>
<td align="left">Start streaming</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-streamon.html">VIDIOC_STREAMOFF</a></td>
<td align="left">Stop streaming</td>
</tr>
</tbody>
</table>
<p>Application sets the format first, then requests and maps buffers from the kernel. Buffers are exchanged between the kernel and the application. When the buffer is enqueued, it will be available for the kernel to capture data to it. When the buffer is dequeued, kernel will not access the buffer and application may read the data. After all buffers are enqueued, application starts the stream. Polling is used to wait for the kernel until it fills the buffer, buffer should not be accessed simultaneously by the kernel and the application. After processing the buffer, application should return it back to the kernel queue. Note that buffers should be properly unmapped by the application after stopping the stream. The video capture process is described in the following diagram.</p>
<div class="figure">
<img src="images/v4l2capture.svg" alt="V4L2 capture" /><p class="caption">V4L2 capture</p>
</div>
<p>Source code for simple video capture is in <a href="#video-capture-example">video capture example</a> appendix. The image format is specified using the little-endian four-character code (FOURCC). V4L2 defines several formats and provides <code>v4l2_fourcc()</code> macro to create a format code from four characters. As described later in the <a href="#graphics-subsystem">graphics subsystem (chapter 2.3)</a> chapter, graphics uses natively the RGB4 format. This format is defined as a single plane with one sample per pixel and four bytes per sample. These bytes represents red, green and blue channel values respectively. Image size is therefore <span class="math">\(width \cdot height \cdot 4\)</span> bytes. Many image sensors however support YUV color-space, for example the YU12 format. This one is defined as three planes, the first plane with one luminance sample per pixel and the second and third plane with one chroma sample per four pixels (2 pixels per row, interleaved). Each sample has one byte, this format is also referenced as YUV 4:2:0 and its image size is <span class="math">\(width \cdot height \cdot 1.5\)</span> bytes. The luminance and chroma of a pixel is defined as</p>
<p><span class="math">\(E_Y = W_R \cdot E_R + (1-W_R-W_B) \cdot E_G + W_B \cdot E_B\)</span>,</p>
<p><span class="math">\(E_{C_r} = \dfrac {0.5 (E_R - E_Y)} {1 - W_R}\)</span>,</p>
<p><span class="math">\(E_{C_b} = \dfrac {0.5 (E_B - E_Y)} {1 - W_B}\)</span>,</p>
<p>where <em>E<sub>R</sub></em>, <em>E<sub>G</sub></em>, <em>E<sub>B</sub></em> are normalized color values and <em>W<sub>R</sub></em>, <em>W<sub>B</sub></em> are their weights. <a href="http://www.itu.int/rec/R-REC-BT.601/en">ITU-R Rec. BT.601</a> <span class="citation">[6]</span> defines weights as 0.299 and 0.114 respectively, it also defines how they are quantized</p>
<p><span class="math">\(Y = 219 E_Y + 16\)</span>,</p>
<p><span class="math">\(C_r = 224 E_{C_r} + 128\)</span>,</p>
<p><span class="math">\(C_b = 224 E_{C_b} + 128\)</span>.</p>
<p>To calculate <em>R</em>, <em>G</em>, <em>B</em> values from <em>Y</em>, <em>Cr</em>, <em>Cb</em> values, inverse formulas must be used</p>
<p><span class="math">\(E_Y = \dfrac {Y - 16} {219}\)</span>,</p>
<p><span class="math">\(E_{C_r} =  \dfrac {C_r - 128} {224}\)</span>,</p>
<p><span class="math">\(E_{C_b} = \dfrac {C_b - 128} {224}\)</span>,</p>
<p><span class="math">\(E_R = E_Y + 2 E_{C_r} (1 - W_R)\)</span>,</p>
<p><span class="math">\(E_G = E_Y - 2 E_{C_r} \dfrac {W_R - {W_R}^2} {W_G} - 2 E_{C_b} \dfrac {W_B - {W_B}^2} {W_G}\)</span>,</p>
<p><span class="math">\(E_B = E_Y + 2 E_{C_b} (1 - W_B)\)</span>.</p>
<p>It should be noted that not all devices may use the BT.601 recommendation, V4L2 refers to it as <code>V4L2_COLORSPACE_SMPTE170M</code> in the <code>VIDIOC_S_FMT</code> request structure. Implementation of the YUV to RGB color-space conversion is most efficient on graphics accelerators, such example is included in <a href="#colorspace-conversion-example">colorspace conversion example</a> appendix. It is written in GLSL for fragment processor, see <a href="#graphics-subsystem">graphics subsystem (chapter 2.3)</a> chapter for further description.</p>
<p>There is a kernel module <code>v4l2loopback</code> which creates a video loop-back device, similar to network loop-back, allowing piping two video applications together. This is very useful not only for testing, but also for implementation of intermediate decoders. <a href="http://gstreamer.freedesktop.org">GStreamer</a> is a powerful multimedia framework widespread in Linux distributions, composed of a core infrastructure and hundreds of plug-ins. This command will create synthetic RGB4 video stream for the application, useful for testing</p>
<p><code class="sourceCode bash"><span class="kw">modprobe</span> v4l2loopback</code><br /><code class="sourceCode bash"><span class="kw">gst-launch</span> videotestsrc pattern=solid-color foreground-color=0xE0F0E0 ! \</code><br /><code class="sourceCode bash"><span class="st">&quot;video/x-raw,format=RGBx,width=800,height=600,framerate=20/1&quot;</span> <span class="kw">\</span></code><br /><code>! v4l2sink device=/dev/video0</code></p>
<p>Texas Instruments distributes a <a href="https://launchpad.net/~tiomap-dev/+archive/omap-trunk">meta package</a> <span class="citation">[7]</span> for their OMAP platform featuring all required modules and DSP firmware. This includes kernel modules for <em>SysLink</em> inter-chip communication library, <em>Distributed Codec Engine</em> library and <em>ducati</em> plug-in for GStreamer. With the meta-package installed, it is very easy and efficient to implement mainstream encoded video formats. For example following command will create GStreamer pipeline to receive video payload over a network socket from an IP camera, decode it and push it to the loop-back device for the application. MPEG-4 AVC (H.264) decoder of the IVA 3 is used in this example.</p>
<p><code class="sourceCode bash"><span class="kw">modprobe</span> v4l2loopback</code><br /><code class="sourceCode bash"><span class="kw">gst-launch</span> udpsrc port=5004 caps=\</code><br /><code class="sourceCode bash"><span class="st">&quot;application/x-rtp,media=video,payload=96,clock-rate=90000,encoding-name=H264&quot;</span> <span class="kw">\</span></code><br /><code>! rtph264depay ! h264parse ! ducatih264dec ! v4l2sink device=/dev/video0</code></p>
<p>On OMAP4460 this would consume only about 15% of the CPU time as the decoding is done by the IVA 3 video accelerator in parallel to the CPU which only passes pointers around and handles synchronization. Output format is NV12 which is similar to YU12 format described earlier, but there is only one chroma plane with two-byte samples, first byte being the U channel and the second byte the V channel, sampling is same 4:2:0. The YUV to RGB color space conversion must take place here, preferably implemented on the GPU as described above.</p>
<p>Cortex-A9 cores on the OMAP4460 also have the NEON co-processor, capable of vector floating point math. Although not very supported by the GCC C compiler, there are many assembly written libraries implementing coders with the NEON acceleration. For example the <a href="http://www.libjpeg-turbo.org/"><em>libjpeg-turbo</em></a> library is implementing the <em>libjpeg</em> interface. It is useful for USB cameras, as the USB throughput is not high enough for raw high definition video, but is sufficient with JPEG coding (as most USB cameras supports JPEG, but does not support H.264). 1080p JPEG stream decoded with this library via its GStreamer plug-in will consume about 90% of the single CPU core time (note that there are two CPU cores available). However, comparable to the AVC, JPEG encoding will cause visible quality degradation in the raw stream (video looks grainy).</p>
<h3 id="implementation"><a href="#implementation">Implementation</a></h3>
<p>The subsystem is implemented as a standalone module designed for synchronous operation within the rendering loop. This implies a constant rendering latency as the multiple of the frame sampling time and the number of buffers in the kernel-space to user-space queue</p>
<p><span class="math">\(latency = \dfrac{n}{f_s}\)</span>.</p>
<p>The minimum number of buffers is 3, however only two buffers are actively used. The first buffer being used for the drawing in the user-space and the second buffer being used for the capture in the kernel-space. The third extra buffer is enqueued in the kernel-space to be used after the second buffer is filled (more than one extra buffer may be used, usually the total number of 4 buffers are used to prevent queue underflow in the case that the rendering loop momentary lingers). The implementation uses <code>select()</code> to wait for the kernel to fill the current buffer and then rotate the buffers in the queue. This means that the sampling rate is the maximum value possible for the capture device hardware and the total latency is <span class="math">\(2\,{{f_s}_{max}}^{-1}\)</span>. The following options are configured by the implementation</p>
<ul>
<li>width, height</li>
<li>format (<code>&quot;RGB4&quot;</code>)</li>
<li>interlacing</li>
</ul>
<p>The video subsystem is capable of using multiple capture devices. Two interlaced video streams per device is also possible. This allows implementation of stereoscopic and multilayer imaging. The video resolution may differ from screen resolution, but the pixel aspect ratio must match.</p>
<h2 id="graphics-subsystem"><a href="#graphics-subsystem">Graphics subsystem</a></h2>
<p>Graphical output in the Linux Kernel is accessible as a framebuffer device <code>/dev/fb</code>. This allows directly writing to a display memory from the application. OMAP platform further extends this interface to support its DSS2 architecture for multiplexing graphical and video systems with display outputs. There is a framebuffer device connected to the PowerVR SGX graphical accelerator with control interface at <code>/sys/class/graphics/fbX</code>, where <code>X</code> is the framebuffer number. The OMAP DSS subsystem is exported at <code>/sys/devices/platform/omapdss</code> as</p>
<ul>
<li><code>/sys/devices/platform/omapdss/overlayX/</code></li>
<li><code>/sys/devices/platform/omapdss/managerX/</code></li>
<li><code>/sys/devices/platform/omapdss/displayX/</code></li>
</ul>
<p>where overlays may read from FB or V4L2 devices, combined in the manager and then linked to a physical display. Input sources can be blended by color keying to create the output image, this is useful for rendering graphical overlays. Individual displays (HDMI, LCD) are also configured by this interface.</p>
<h3 id="opengl-es-2.0-api"><a href="#opengl-es-2.0-api">OpenGL ES 2.0 API</a></h3>
<p>There is a standardized library for interfacing with graphical accelerators maintained by Khronos group called OpenGL. Its recent version targeted for embedded systems is <a href="http://www.khronos.org/opengles/2_X">OpenGL ES 2.0</a> <span class="citation">[8]</span> <span class="citation">[9]</span>, implemented by majority of hardware developers. It is also supported by the multi-platform Mesa3D library, so it will run also on desktop computer, either emulated by CPU or partially accelerated depending on available hardware. OpenGL ES 2.0 is implemented in two parts, the kernel module and user-space libraries <em>EGL</em> and <em>GLESv2</em>. Its implementation will be platform specific, however the application interface is the same. Functions names in the API are prefixed with <em>gl</em> and suffixed by argument types, used for overloading. EGL library is used for initialization of the OpenGL context, following function are available in the API:</p>
<table>
<caption>EGL function for OpenGL ES initialization</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>eglGetDisplay()</code></td>
<td align="left">Select display</td>
</tr>
<tr class="even">
<td align="left"><code>eglInitialize()</code></td>
<td align="left">Initialize display</td>
</tr>
<tr class="odd">
<td align="left"><code>eglBindAPI()</code></td>
<td align="left">Select OpenGL API version</td>
</tr>
<tr class="even">
<td align="left"><code>eglChooseConfig()</code></td>
<td align="left">Select configuration options</td>
</tr>
<tr class="odd">
<td align="left"><code>eglCreateWindowSurface()</code></td>
<td align="left">Create drawable surface (bound to native window)</td>
</tr>
<tr class="even">
<td align="left"><code>eglCreateContext()</code></td>
<td align="left">Create OpenGL context</td>
</tr>
<tr class="odd">
<td align="left"><code>eglMakeCurrent()</code></td>
<td align="left">Activate context and surface</td>
</tr>
</tbody>
</table>
<p></p>
<p>Graphical pipeline is described in the diagram below. The pipeline is programmable, the program runs on the GPU, while OpenGL API is used for communication with the application running on CPU.</p>
<div class="figure">
<img src="images/pipeline.svg" alt="OpenGL ES pipeline" /><p class="caption">OpenGL ES pipeline</p>
</div>
<p>Programs are compiled from source and written in the GLSL language, each program consists of vertex and fragment shader. Following function are available in the API:</p>
<table>
<caption>OpenGL functions for working with shader programs</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>glCreateShader()</code></td>
<td align="left">Create shader</td>
</tr>
<tr class="even">
<td align="left"><code>glShaderSource()</code></td>
<td align="left">Load shader source</td>
</tr>
<tr class="odd">
<td align="left"><code>glCompileShader()</code></td>
<td align="left">Compile shader from loaded source</td>
</tr>
<tr class="even">
<td align="left"><code>glDeleteShader()</code></td>
<td align="left">Delete shader</td>
</tr>
<tr class="odd">
<td align="left"><code>glShaderBinary()</code></td>
<td align="left">Load shader from binary data</td>
</tr>
<tr class="even">
<td align="left"><code>glCreateProgram()</code></td>
<td align="left">Create program</td>
</tr>
<tr class="odd">
<td align="left"><code>glAttachShader()</code></td>
<td align="left">Add shader to program</td>
</tr>
<tr class="even">
<td align="left"><code>glLinkProgram()</code></td>
<td align="left">Link shaders in program</td>
</tr>
<tr class="odd">
<td align="left"><code>glUseProgram()</code></td>
<td align="left">Switch between multiple programs</td>
</tr>
<tr class="even">
<td align="left"><code>glDeleteProgram()</code></td>
<td align="left">Delete program</td>
</tr>
<tr class="odd">
<td align="left"><code>glGetUniformLocation()</code></td>
<td align="left">Access uniform variable defined in shader</td>
</tr>
<tr class="even">
<td align="left"><code>glGetAttribLocation()</code></td>
<td align="left">Access attribute variable defined in shader</td>
</tr>
</tbody>
</table>
<p>The vertex shader processes geometry defined as array of verticies, it is executed per vertex. Result verticies are rasterized into fragments and then fragment shader is executed per fragment. Result fragments are then written into the framebuffer, each shader execution is done in parallel. Following figure shows how data are processed by the shader program.</p>
<div class="figure">
<img src="images/shaders.svg" alt="OpenGL ES shader program" /><p class="caption">OpenGL ES shader program</p>
</div>
<p>GPU uses its own memory, it may be physically shared with the system memory, but is not directly accessible by the application. Verticies are stored in the GPU memory in vertex buffer objects (VBO), following API function are available:</p>
<table>
<caption>OpenGL functions for working with VBOs</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>glGenBuffers()</code></td>
<td align="left">Create vertex buffer object</td>
</tr>
<tr class="even">
<td align="left"><code>glDeleteBuffers()</code></td>
<td align="left">Destroy vertex buffer object</td>
</tr>
<tr class="odd">
<td align="left"><code>glBufferData()</code></td>
<td align="left">Load vertex data into VBO</td>
</tr>
<tr class="even">
<td align="left"><code>glBindBuffer()</code></td>
<td align="left">Bind VBO to attribute array</td>
</tr>
<tr class="odd">
<td align="left"><code>glVertexAttribPointer()</code></td>
<td align="left">Specify attribute array</td>
</tr>
<tr class="even">
<td align="left"><code>glEnableVertexAttribArray()</code></td>
<td align="left">Enable attribute array</td>
</tr>
</tbody>
</table>
<p>Shader programs have three types of variables, attributes, uniforms and varyings. Attributes are read from the attribute array which is bound to the VBO. Each vertex from the array is processed separately by each shader execution, having its value accessible by the attribute variables. Uniforms can be assigned directly by the application using <code>glUniform()</code>, they are read-only by the shader and they are shared by each execution. Varyings are used to pass variables from vertex shader to fragment shader, they are interpolated between verticies during the rasterization. Fragment shaders may use textures to to calculate fragment color, textures are stored in the GPU memory and loaded by the application.</p>
<p></p>
<p>Following functions are available in the API:</p>
<table>
<caption>OpenGL functions for working with textures</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>glGenTextures()</code></td>
<td align="left">Create texture object</td>
</tr>
<tr class="even">
<td align="left"><code>glTexImage2D()</code></td>
<td align="left">Load pixel data into texture object</td>
</tr>
<tr class="odd">
<td align="left"><code>glTexSubImage2D()</code></td>
<td align="left">Load partial pixel data</td>
</tr>
<tr class="even">
<td align="left"><code>glTexParameter()</code></td>
<td align="left">Set parameter</td>
</tr>
<tr class="odd">
<td align="left"><code>glBindTexture()</code></td>
<td align="left">Bind texture to active unit</td>
</tr>
<tr class="even">
<td align="left"><code>glActiveTexture()</code></td>
<td align="left">Select texture unit</td>
</tr>
<tr class="odd">
<td align="left"><code>glDeleteTextures()</code></td>
<td align="left">Delete texture</td>
</tr>
</tbody>
</table>
<p>Textures are bound to the texture units which may be accessed from the shader program. If the fragment shader needs to work with multiple textures, each texture needs to be loaded into a different texturing unit. The typical drawing loop is:</p>
<ul>
<li>select shader program</li>
<li>assign uniform variables</li>
<li>bind textures to texturing units</li>
<li>bind VBOs to attribute arrays</li>
<li>start processing with <code>glDrawArrays()</code></li>
<li>swap framebuffers and repeat</li>
</ul>
<h3 id="opengl-es-shading-language"><a href="#opengl-es-shading-language">OpenGL ES Shading Language</a></h3>
<p>Shader programs are designed to be executed in parallel, over large blocks of data. The language is similar to C, the minimal structure of the vertex shader is:</p>
<pre class="sourceCode c"><code class="sourceCode c">attribute vec4 vertex;
varying vec2 texcoord;

<span class="dt">void</span> main()
{
  gl_Position = vec4(vertex.xy, <span class="dv">0</span>, <span class="dv">1</span>);
  texcoord = vertex.zw;
}</code></pre>
<p></p>
<p>and the fragment shader:</p>
<pre class="sourceCode c"><code class="sourceCode c">uniform sampler2D texture;
varying vec2 texcoord;

<span class="dt">void</span> main()
{
  gl_FragColor = texture2D(texture, texcoord);
}</code></pre>
<p>This vertex shader takes one vertex attribute as a vector, where first two fields are display coordinates and the second two fields are texture coordinates. It defines varying variable to pass texture coordinates to the fragment shader. The <code>gl_Position</code> is special variable resembling the output of the vertex shader (the vertex position). The fragment shader access texturing unit passed as a uniform variable with the interpolated texture coordinates from the vertex shader. Result is written to <code>gl_FragColor</code> which is a special variable resembling the output of the fragment shader (the RGBA color). GLSL supports vector and matrix types, for example <code>vec4</code> is the four element vector and <code>mat3</code> is the 3x3 matrix. Vectors may be combined freely for example</p>
<p><code class="sourceCode c">vec4(vec4(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>).xy, <span class="dv">4</span>, <span class="dv">5</span>) * <span class="fl">1.5</span></code></p>
<p>will create vector (0, 1, 2, 3), take its first two fields (x, y) to create vector (0, 1, 4, 5) and then multiply by scalar resulting in vector (0, 1.5, 6, 7.5). GLSL also features standard mathematical functions such as trigonometry, exponential, geometric, vector or matrix. The <code>sampler2D</code> type refers to the texturing unit, to access its texture function <code>texture2D()</code> is used. The way by which samplers calculates the texture color is determined by the parameters set through the API. For example having two pixel texture with one black and one white pixel with setting</p>
<p><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);</code><br /><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);</code><br /><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);</code><br /><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);</code></p>
<p>will cause linear color mapping and therefore black-white color gradient. However setting</p>
<p><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT);</code><br /><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT);</code><br /><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);</code><br /><code class="x">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);</code></p>
<p>will create chessboard pattern. These setting are done per texture object and reflected by texturing unit to which the texture is bound.</p>
<h3 id="truetype-font-rendering"><a href="#truetype-font-rendering">TrueType font rendering</a></h3>
<p>In order to be able to render TrueType vector fonts, each glyph needs to be pre-rasterized first. The best method to achieve this is to create glyph atlas texture with all glyphs needed and then generate strings as VBOs with proper texture coordinates for each character. To utilize unicode support, this atlas needs to be appended by newly requested characters in real-time as there are too many glyphs to be pre-rasterized. The <a href="http://www.freetype.org/">FreeType2</a> library can rasterize glyphs from the TrueType font file on the fly. These glyphs are in fact alpha maps to be processed by the fragment shader:</p>
<p><code class="sourceCode c">gl_FragColor = color * texture2D(texture, texcoord).a;</code></p>
<p>or</p>
<p><code class="sourceCode c">gl_FragColor = vec4(color.rgb, texture2D(texture, texcoord).a);</code></p>
<p>if the alpha blending is enabled with</p>
<p><code class="sourceCode c">glEnable(GL_BLEND);</code><br /><code class="sourceCode c">glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA);</code></p>
<p>where <code>color</code> is the text color.</p>
<p>The FreeType library loads the font faces from <code>.ttf</code> files and has following API:</p>
<table>
<caption>FreeType API</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>FT_Init_FreeType()</code></td>
<td align="left">Initialize the library</td>
</tr>
<tr class="even">
<td align="left"><code>FT_New_Face()</code></td>
<td align="left">Load font face from file</td>
</tr>
<tr class="odd">
<td align="left"><code>FT_Select_Charmap()</code></td>
<td align="left">Set encoding (Unicode)</td>
</tr>
<tr class="even">
<td align="left"><code>FT_Set_Pixel_Sizes()</code></td>
<td align="left">Set font size</td>
</tr>
<tr class="odd">
<td align="left"><code>FT_Load_Char()</code></td>
<td align="left">Rasterize single glyph</td>
</tr>
<tr class="even">
<td align="left"><code>FT_Done_FreeType()</code></td>
<td align="left">Release resources</td>
</tr>
</tbody>
</table>
<p>The glyphs are loaded to texture with <code>glTexSubImage2D()</code>. To change the font size it is better to rasterize new glyph with <code>FT_Load_Char()</code>, then trying to scale the glyph in the fragment shader. To achieve best results, there should be 1:1 mapping between glyph pixels and OpenGL fragments and blending should be enabled. The library also supports kerning and other features to make the result text better or to create special effects. Special care needs to be taken into account about pixel alignment when rendering text. Vertex shader must ensure that each glyph verticies are aligned without pixel fractions. The typical example is the centering of text with even pixel width, which causes verticies to be aligned with 0.5 pixel offset. This causes major aliasing artifacts during rasterization and can be prevented by subtracting the fragment coordinate fractional part in the vertex shader.</p>
<h3 id="texture-streaming-extensions"><a href="#texture-streaming-extensions">Texture streaming extensions</a></h3>
<p>Loading textures the standard way causes copying the pixel buffer to the texture memory, which is very inefficient if the texture needs to be changed often. This is typical to streaming video through the GPU. There is a <code>KHR_image_base</code> extension for the EGL and a <code>OES_EGL_image</code> extension for the OpenGL ES defined in <code>EGL/eglext.h</code> and <code>GLES2/gl2ext.h</code> respectively. These extensions are platform specific, this text refers to the Texas Instruments implementation. The <code>EGLImage</code> offers a way to map images in the EGL API to be accessed in OpenGL as <code>GL_TEXTURE_EXTERNAL_OES</code> textures. This works as memory mapping and no copying is done whatsoever, synchronization is handled by the application. The mapping is created by</p>
<pre class="sourceCode c"><code class="sourceCode c">EGLImageKHR img =
  eglCreateImageKHR(dpy, EGL_NO_CONTEXT, EGL_RAW_VIDEO_TI, ptr, attr);
glEGLImageTargetTexture2DOES(GL_TEXTURE_EXTERNAL_OES, (GLeglImageOES)img);
glBindTexture(GL_TEXTURE_EXTERNAL_OES, myTexture);</code></pre>
<p>where <code>dpy</code> is the active EGL display, <code>ptr</code> is pointer to the video buffer and <code>attr</code> is array of configuration options. This extension is also able to perform YUV to RGB color-space conversion in the background.</p>
<h3 id="system-integration"><a href="#system-integration">System integration</a></h3>
<p>Linux distributions usually use common framework for graphical applications and user interfaces. To integrate this project into a high level graphical user interface application, the <code>eglCreateWindowSurface()</code> function binds the drawing surface to a specific native window. For windowless drawing (such as when the application runs directly on top of the kernel) a <code>NULL</code> window is used. The application is built as a plug-in object, that may be bound to higher level framework such as <em>XLib</em>, <em>GTK+</em> or <em>Qt</em>, which defines a specific window area and pass this to the <code>eglCreateWindowSurface()</code> function. For example, the next code snippet shows simple <em>XLib</em> integration.</p>
<pre class="sourceCode c"><code class="sourceCode c">Display *display = XOpenDisplay(NULL);
<span class="dt">unsigned</span> <span class="dt">long</span> color = BlackPixel(display, <span class="dv">0</span>);
Window root = RootWindow(display, <span class="dv">0</span>);
Window window =
  XCreateSimpleWindow(display, root, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">800</span>, <span class="dv">600</span>, <span class="dv">0</span>, color, color);
XMapWindow(display, window);
XFlush(display);

EGLConfig egl_config; <span class="co">// TODO: eglChooseConfig()</span>
EGLDisplay egl_display = eglGetDisplay(EGL_DEFAULT_DISPLAY);
EGLSurface egl_surface =
  eglCreateWindowSurface(egl_display, egl_config, window, NULL);</code></pre>
<p>The same <em>GTK+</em> application would be</p>
<pre class="sourceCode c"><code class="sourceCode c">GtkWidget *main_window = gtk_window_new(GTK_WINDOW_TOPLEVEL);
GtkWidget *video_window = gtk_drawing_area_new();
gtk_widget_set_double_buffered(video_window, FALSE);
gtk_container_add(GTK_CONTAINER(main_window), video_window);
gtk_window_set_default_size(GTK_WINDOW(main_window), <span class="dv">640</span>, <span class="dv">480</span>);
gtk_widget_show_all(main_window);

EGLConfig egl_config; <span class="co">// TODO: eglChooseConfig()</span>
EGLDisplay egl_display = eglGetDisplay(EGL_DEFAULT_DISPLAY);
GdkWindow *window = gtk_widget_get_window(video_window);
EGLSurface egl_surface =
  eglCreateWindowSurface(egl_display, egl_config, GDK_WINDOW_XID(window), NULL);</code></pre>
<p>And the the <em>Qt</em> application</p>
<pre class="sourceCode cpp"><code class="sourceCode cpp">QApplication app(<span class="dv">0</span>, NULL);
QWidget window;
window.resize(<span class="dv">800</span>, <span class="dv">600</span>);
window.show();

EGLConfig egl_config; <span class="co">// TODO: eglChooseConfig()</span>
EGLDisplay egl_display = eglGetDisplay(EGL_DEFAULT_DISPLAY);
EGLSurface egl_surface =
  eglCreateWindowSurface(egl_display, egl_config, window.winId(), NULL);</code></pre>
<p>The <code>egl_config</code> structure is used to configure the OpenGL context, it is a NULL terminated list of attribute-value pairs. The following table shows some valid attributes.</p>
<table>
<caption>EGL attributes</caption>
<thead>
<tr class="header">
<th align="left"><strong>Attribute</strong></th>
<th align="left"><strong>Value</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>EGL_DEPTH_SIZE</code></td>
<td align="left">Depth buffer size, in bits</td>
</tr>
<tr class="even">
<td align="left"><code>EGL_RED_SIZE</code></td>
<td align="left">Size of the red component of the color buffer, in bits</td>
</tr>
<tr class="odd">
<td align="left"><code>EGL_GREEN_SIZE</code></td>
<td align="left">Size of the green component of the color buffer, in bits</td>
</tr>
<tr class="even">
<td align="left"><code>EGL_BLUE_SIZE</code></td>
<td align="left">Size of the blue component of the color buffer, in bits</td>
</tr>
<tr class="odd">
<td align="left"><code>EGL_ALPHA_SIZE</code></td>
<td align="left">Size of the alpha component of the color buffer, in bits</td>
</tr>
<tr class="even">
<td align="left"><code>EGL_RENDERABLE_TYPE</code></td>
<td align="left"><code>EGL_OPENGL_BIT</code>, <code>EGL_OPENGL_ES_BIT</code>, <code>EGL_OPENGL_ES2_BIT</code></td>
</tr>
<tr class="odd">
<td align="left"><code>EGL_SURFACE_TYPE</code></td>
<td align="left"><code>EGL_PBUFFER_BIT</code>, <code>EGL_PIXMAP_BIT</code>, <code>EGL_WINDOW_BIT</code></td>
</tr>
</tbody>
</table>
<h3 id="implementation-1"><a href="#implementation-1">Implementation</a></h3>
<p>The graphics subsystem is implemented as a widget oriented drawing library. The widget is a standalone drawable object with a common drawing interface consisting of</p>
<ul>
<li>drawable type (geometry primitive, vector text, image)</li>
<li>vertex buffer object</li>
<li>vertex number</li>
<li>texture object</li>
<li>drawing mode type (lines, surfaces)</li>
<li>drawing color and texture color mask</li>
</ul>
<p>A common drawing function is implemented for all widgets, arguments of this function are: translation (x, y screen coordinates), scale (0-1) and rotation (0-<span class="math">\(2\pi\)</span>). The translation coordinates are normalized before rendering. The following vertex shader is used to provide these calculations</p>
<pre class="sourceCode c"><code class="sourceCode c">attribute vec4 coord;
uniform vec2 offset;
uniform vec2 scale;
uniform <span class="dt">float</span> rot;
varying vec2 texpos;
<span class="dt">void</span> main()
{
  <span class="dt">float</span> sinrot = sin(rot);
  <span class="dt">float</span> cosrot = cos(rot);
  vec2 pos = vec2(coord.x * cosrot - coord.y * sinrot,
                  coord.x * sinrot + coord.y * cosrot);
  gl_Position = vec4(pos * scale + offset, <span class="dv">0</span>, <span class="dv">1</span>);
  texpos = coord.zw;
}</code></pre>
<p>The respective fragment shader is</p>
<pre class="sourceCode c"><code class="sourceCode c">uniform vec4 color;
uniform vec4 mask;
uniform sampler2D tex;
varying vec2 texpos;
<span class="dt">void</span> main()
{
  gl_FragColor = texture2D(tex, texpos) * mask + color;
}</code></pre>
<h2 id="inertial-measurement-subsystem"><a href="#inertial-measurement-subsystem">Inertial measurement subsystem</a></h2>
<p>Application needs to know its spatial orientation for rendering, there are three devices which may provide such information, gyroscope, compass (magnetometer) and accelerometer. Hardware details about these devices are in the <a href="#hardware">chapter 3</a>.</p>
<h3 id="industrial-io-module"><a href="#industrial-io-module">Industrial I/O module</a></h3>
<p>A relatively young kernel module <code>iio</code> has been implemented in recent kernels to provide standardized support for sensors and analog converters typically connected by I<sup>2</sup>C bus. While many device drivers are still in staging tree, to core module is ready for production code. Subsystem provides device structure mapped in <code>sysfs</code>, typically available at <code>/sys/bus/iio/devices/</code>. Device are implemented usually on top of the <code>i2c-dev</code> driver and registered as <code>/sys/bus/iio/devices/iio:deviceX</code>, where X is the device number and the device name may be obtained by</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:deviceX/name</code></p>
<p>There are many possible channels, named by the value type they represents. To read an immediate value, for example from an ADC channel 1</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:deviceX/in_voltage1_raw</code><br /><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:deviceX/in_voltage_scale</code></p>
<p>where the result value in volts is <span class="math">\(raw \cdot scale\)</span>. However, being easy, this is not efficient, buffers have been implemented to stream measured data to the application. Buffer uses device file named after the <code>iio</code> device, e.g. <code>/dev/iio:deviceX</code>. To stream data through the buffer, driver needs to have control over the timing, triggers have been implemented for this purpose. They are accessible as <code>/sys/bus/iio/devices/triggerX</code>, where X is the trigger number and its name may be obtained by</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/triggerX/name</code></p>
<p>Software trigger may be created by</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/iio_sysfs_trigger/add_trigger</code></p>
<p>and triggered by application</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/trigger0/trigger_now</code></p>
<p>Name of this trigger is <code>sysfstrigX</code>, where X is the trigger number. Hardware triggers are also implemented, both GPIO and timer based triggers. Devices may implement triggers themselves, providing for example the data ready trigger. Device triggers are generally named as <code>name-devX</code>, where <code>name</code> is device name and <code>X</code> is device number. To use trigger with the buffer use</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> <span class="st">&quot;triggername&quot;</span> <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:deviceX/trigger/current_trigger</code></p>
<p>where <code>triggername</code> is the name of the trigger, for example <code>adc-dev0</code> will be the device trigger for the ADC. Data are measured in specific channels, they are defined in <code>/sys/bus/iio/devices/iio:device0/scan_elements</code>. Channels must be enabled for buffering individually, for example</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltage1_en</code><br /><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltage2_en</code></p>
<p>will enable ADC channels 1 and 2. Buffer itself can be started by</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 256 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:deviceX/buffer/length</code><br /><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:deviceX/buffer/enabled</code></p>
<p>this will start streaming data to the device file. Data are formatted in packets, each packed consists of per-channel values and is terminated by 8 byte time-stamp of the sample. Order of the channels in the buffer can be obtained by</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltageX_index</code></p>
<p>which reads index of the specified channel. Data format of this channel is</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltageX_type</code></p>
<p>which reads encoded string, for example <code>le:u10/16&gt;&gt;0</code>, where <code>le</code> means little-endian, <code>u</code> means unsigned, <code>10</code> is the number of relevant bits while <code>16</code> is the number of actual bits and <code>0</code> is the number of right shifts needed.</p>
<p>Following channels are needed by the application:</p>
<ul>
<li><code>anglvel_x</code></li>
<li><code>anglvel_y</code></li>
<li><code>anglvel_z</code></li>
<li><code>accel_x</code></li>
<li><code>accel_y</code></li>
<li><code>accel_z</code></li>
<li><code>magn_x</code></li>
<li><code>magn_y</code></li>
<li><code>magn_z</code></li>
<li><code>pressure</code></li>
</ul>
<p>representing measurements from gyroscope, accelerometer, magnetometer and barometer respectively. The barometer measurements are used for altitude calculation in satellite navigation subsystem and are not taking any part in DCM algorithm.</p>
<h3 id="dcm-algorithm"><a href="#dcm-algorithm">DCM algorithm</a></h3>
<p>Equations needed to calculate device attitude have been derived from <span class="citation">[10]</span>. Gyroscope measures angular speed around device axes, it offers high differential precision and fast sampling rate, however it suffers slight zero offset error. Device attitude can be obtained simply by integrating measured angular rates, provided that initial attitude is known. The angular rate is defined as</p>
<p><span class="math">\(\overrightarrow{\omega_g} = \frac{\mathrm{d}}{\mathrm{d}t} \overrightarrow{\Phi}_{(t)}\)</span>,</p>
<p>so the angular displacement between last two samples is</p>
<p><span class="math">\(\left [\Phi_x, \Phi_y, \Phi_z \right ] = \left [ \omega_x, \omega_y, \omega_z \right ] \cdot _\Delta t\)</span>.</p>
<p>This can be described as a rotation</p>
<p><span class="math">\(\mathbf{R}_{gyro} = \begin{bmatrix} \cos(\Phi_z) &amp; -\sin(\Phi_z) &amp; 0 \\  \sin(\Phi_z) &amp; \cos(\Phi_z) &amp; 0 \\  0 &amp; 0 &amp; 1 \end{bmatrix} \times \begin{bmatrix} \cos(\Phi_y) &amp; 0 &amp; \sin(\Phi_y) \\  0 &amp; 1 &amp; 0 \\  -\sin(\Phi_y) &amp; 0 &amp; \cos(\Phi_y) \end{bmatrix} \times \begin{bmatrix} 1 &amp; 0 &amp; 0 \\  0 &amp; \cos(\Phi_x) &amp; -\sin(\Phi_x) \\  0 &amp; \sin(\Phi_x) &amp; \cos(\Phi_x) \end{bmatrix}\)</span>.</p>
<p>With <span class="math">\(_\Delta t\)</span> close to zero a small-angle approximation may be used to simplify <span class="math">\(\cos(x)=1\)</span>, <span class="math">\(\sin(x)=x\)</span></p>
<p><span class="math">\(\mathbf{R}_{gyro} \doteq \begin{bmatrix} 1 &amp; \Phi_x \Phi_y + \Phi_z &amp; \Phi_x \Phi_z - \Phi_y \\ - \Phi_z &amp; 1 - \Phi_x \Phi_y \Phi_z &amp; \Phi_x + \Phi_y \Phi_z \\ \Phi_y &amp; - \Phi_x &amp; 1 \end{bmatrix}\)</span>.</p>
<p>Let us define the directional cosine matrix describing device attitude</p>
<p><span class="math">\(\mathbf{DCM} = \begin{bmatrix} \widehat{\mathbf{I}}\cdot \widehat{\mathbf{x}} &amp; \widehat{\mathbf{I}}\cdot \widehat{\mathbf{y}} &amp; \widehat{\mathbf{I}}\cdot \widehat{\mathbf{z}} \\ \widehat{\mathbf{J}}\cdot \widehat{\mathbf{x}} &amp; \widehat{\mathbf{J}}\cdot \widehat{\mathbf{y}} &amp; \widehat{\mathbf{J}}\cdot \widehat{\mathbf{z}} \\  \widehat{\mathbf{K}}\cdot \widehat{\mathbf{x}} &amp; \widehat{\mathbf{K}}\cdot \widehat{\mathbf{y}} &amp; \widehat{\mathbf{K}}\cdot \widehat{\mathbf{z}} \end{bmatrix} = \begin{bmatrix} \widehat{\mathbf{I}}_{xyz} \\ \widehat{\mathbf{J}}_{xyz} \\ \widehat{\mathbf{K}}_{xyz} \end{bmatrix}\)</span>,</p>
<p>where <span class="math">\(\widehat{\mathbf{I}}\)</span> points to the north, <span class="math">\(\widehat{\mathbf{J}}\)</span> points to the east, <span class="math">\(\widehat{\mathbf{K}}\)</span> points to the ground and therefore <span class="math">\(\widehat{\mathbf{I}} = \widehat{\mathbf{J}} \times \widehat{\mathbf{K}}\)</span>. The figure bellow shows the vector relations</p>
<div class="figure">
<img src="images/dcmvectors.svg" alt="DCM vectors" /><p class="caption">DCM vectors</p>
</div>
<p>Roll, pitch and yaw angels in this matrix are</p>
<p><span class="math">\(\gamma = - \arctan_2 \left ( \dfrac {\mathbf{DCM}_{32}}{\mathbf{DCM}_{33}} \right )\)</span>,</p>
<p><span class="math">\(\beta = \arcsin (\mathbf{DCM}_{31})\)</span>,</p>
<p><span class="math">\(\alpha = - \arctan_2 \left ( \dfrac {\mathbf{DCM}_{21}}{\mathbf{DCM}_{11}} \right )\)</span>.</p>
<p>DCM can be computed by applying consecutive rotations over time</p>
<p><span class="math">\(\mathbf{DCM}_{(t)}  =  \mathbf{R}_{gyro(t)} \times \mathbf{DCM}_{(t-1)}\)</span>.</p>
<p>If the sampling rate is high enough (over 1kHz at least), this method is very accurate and has good dynamics over short periods of time, but in longer runs errors integrated during processing will cause serious drift (both numerical errors and zero offset errors). To mitigate these problems accelerometer and compass has to be used to provide the initial attitude and to fix the drift over time. Accelerometer measures external mechanical forces applied to the device together with gravitational force. However precision of these devices are generally worse and they have slower sampling rates. If there are no extern forces, it will measure the gravitational vector directly, thus providing the third row of the DCM</p>
<p><span class="math">\(\overrightarrow{\mathbf{a}}_{acc} = g~ \widehat{\mathbf{K}}_{xyz} + \dfrac{\overrightarrow{\mathbf{F}}}{m}\)</span>,</p>
<p><span class="math">\(\overrightarrow{\mathbf{F}} = 0 ~\rightarrow~ \widehat{\mathbf{K}}_{xyz} =  \dfrac {\overrightarrow{\mathbf{a}}_{acc}} {\left | \overrightarrow{\mathbf{a}}_{acc} \right |}\)</span>.</p>
<p>When there is an external force <span class="math">\(\overrightarrow{\mathbf{F}}\)</span> applied, which is not parallel and has significant magnitude relative to gravitational force <span class="math">\(m\,g\,\widehat{\mathbf{K}}_{xyz}\)</span>, measurements will degrade rapidly reaching singularity during the free fall (<span class="math">\(\left | \overrightarrow{\mathbf{a}}_{acc} \right | = 0\)</span>). This error may be corrected by using device speed measured by satellite navigation system with high sample rate (over 10Hz)</p>
<p><span class="math">\(\widehat{\mathbf{K}}_{xyz} = \dfrac {\overrightarrow{\mathbf{a}}_{acc} - \frac{\mathrm{d}}{\mathrm{d}t} {\overrightarrow{\mathbf{v}}_{SAT}}} {g}\)</span>.</p>
<p>Magnetometer has similar properties, it measures magnetic flux density of the field the device is within. This should ideally result in a vector pointing to the north, therefore providing the first row of the DCM</p>
<p><span class="math">\(\widehat{\mathbf{I}}_{xyz} = \dfrac {\overrightarrow{\mathbf{B}}_{corr}} {\left | \overrightarrow{\mathbf{B}}_{corr} \right |}\)</span>.</p>
<p>Magnetometers have even slower sampling rates and far worse precision as the Earth field is distorted by nearby metal objects. This magnetic deviation can be divided into hard-iron and soft-iron effects. Hard-iron distortion is caused by materials that produces magnetic field, that is added to the Earth magnetic field. Vector of this field can be subtracted to compensate this error</p>
<p><span class="math">\(\overrightarrow{\mathbf{B}}_{corr1} = \overrightarrow{\mathbf{B}}_{mag} - \frac{1}{2} \left [ \min(B_x) + \max(B_x), \min(B_y) + \max(B_y), \min(B_z) + \max(B_z) \right ]\)</span>.</p>
<p>The soft-iron distortion is caused by soft magnetic materials, which reshapes the field in a way that is not simply additive. It may be observed as an ellipse when the device is rotated around and the measured values are plotted. Compensating for these effects is involves remapping this ellipse back to the sphere. This is computation intensive and as soft-iron effects are usually weak (up to few degrees), it may be omitted.</p>
<p>Further more the magnetic field of the Earth itself does not point to the geographic north, but is rotated by an angle specific to the location on the Earth surface. Magnetic inclination is the vertical portion of this rotation causing magnetic vector to incline to the ground, it may be fixed by using measurements from the accelerometer to make the magnetic vector perpendicular to the gravitational vector</p>
<p><span class="math">\(\overrightarrow{\mathbf{B}}_{corr2} = \widehat{\mathbf{K}}_{xyz} \times \overrightarrow{\mathbf{B}}_{mag} \times \widehat{\mathbf{K}}_{xyz}\)</span>.</p>
<p>Magnetic declination (sometimes referred as magnetic variation) is the horizontal portion of this rotation and is sometimes provided by the satellite navigation systems. To correct for this error, measured values have to be rotated by the inverse angle</p>
<p><span class="math">\(\overrightarrow{\mathbf{B}}_{corr3} = \overrightarrow{\mathbf{B}}_{mag} \times \begin{bmatrix} \cos(var) &amp; - \sin(var) \\ \sin(var) &amp; \cos(var) \end{bmatrix}\)</span>.</p>
<p>By combination of the corrected results from accelerometer and magnetometer complete DCM can be calculated. Weighted average should be used, in real-time this yields</p>
<p><span class="math">\(\mathbf{DCM}_{(t)}  =  W_{gyro}~ (\mathbf{R}_{gyro} \times \mathbf{DCM}_{(t-1)}) + (1 - W_{gyro}) \begin{bmatrix} \widehat{\mathbf{I}}_{xyz} \\ \widehat{\mathbf{K}}_{xyz} \times \widehat{\mathbf{I}}_{xyz} \\ \widehat{\mathbf{K}}_{xyz} \end{bmatrix}\)</span>,</p>
<p>where <span class="math">\(\widehat{\mathbf{I}}_{xyz}\)</span> and <span class="math">\(\widehat{\mathbf{K}}_{xyz}\)</span> are calculated from magnetometer and accelerometer measurements. <em>W<sub>gyro</sub></em> is the weight of the gyroscope measurement, it must be estimated by trial and error to mitigate its drift but not add too much noise. The DCM rows needs to be normalized after computing the average, as the equation does not ensure it.</p>
<h3 id="implementation-2"><a href="#implementation-2">Implementation</a></h3>
<p>The implementation uses standard I/O operations on the device file to read measured values in predefined format. This is done synchronously in a loop running in its own thread, together with the DCM algorithm. The <code>select()</code> function is used to synchronize with the measurement rate. The application asynchronously reads actual state values from the rendering loop as illustrated in a diagram below.</p>
<div class="figure">
<img src="images/dcmthreads.svg" alt="DCM algorithm thread" /><p class="caption">DCM algorithm thread</p>
</div>
<p>A special care must be taken during the quantization as some values may be in different byte encoding. Device specific scaling and offset must also be applied, together with all measurement corrections. The matrix multiplication is implemented directly as shown in the following code sample</p>
<pre class="sourceCode c"><code class="sourceCode c">dcm[<span class="dv">0</span>] = dcm[<span class="dv">0</span>] +
         dcm[<span class="dv">3</span>] * (phi[<span class="dv">0</span>] * phi[<span class="dv">1</span>] + phi[<span class="dv">2</span>]) +
         dcm[<span class="dv">6</span>] * (phi[<span class="dv">0</span>] * phi[<span class="dv">2</span>] - phi[<span class="dv">1</span>]);
dcm[<span class="dv">1</span>] = dcm[<span class="dv">1</span>] +
         dcm[<span class="dv">4</span>] * (phi[<span class="dv">0</span>] * phi[<span class="dv">1</span>] + phi[<span class="dv">2</span>]) +
         dcm[<span class="dv">7</span>] * (phi[<span class="dv">0</span>] * phi[<span class="dv">2</span>] - phi[<span class="dv">1</span>]);
dcm[<span class="dv">2</span>] = dcm[<span class="dv">2</span>] +
         dcm[<span class="dv">5</span>] * (phi[<span class="dv">0</span>] * phi[<span class="dv">1</span>] + phi[<span class="dv">2</span>]) +
         dcm[<span class="dv">8</span>] * (phi[<span class="dv">0</span>] * phi[<span class="dv">2</span>] - phi[<span class="dv">1</span>]);
dcm[<span class="dv">3</span>] = dcm[<span class="dv">0</span>] * -phi[<span class="dv">2</span>] +
         dcm[<span class="dv">3</span>] * (<span class="dv">1</span> - phi[<span class="dv">0</span>] * phi[<span class="dv">1</span>] * phi[<span class="dv">2</span>]) +
         dcm[<span class="dv">6</span>] * (phi[<span class="dv">0</span>] + phi[<span class="dv">1</span>] * phi[<span class="dv">2</span>]);
dcm[<span class="dv">4</span>] = dcm[<span class="dv">1</span>] * -phi[<span class="dv">2</span>] +
         dcm[<span class="dv">4</span>] * (<span class="dv">1</span> - phi[<span class="dv">0</span>] * phi[<span class="dv">1</span>] * phi[<span class="dv">2</span>]) +
         dcm[<span class="dv">7</span>] * (phi[<span class="dv">0</span>] + phi[<span class="dv">1</span>] * phi[<span class="dv">2</span>]);
dcm[<span class="dv">5</span>] = dcm[<span class="dv">2</span>] * -phi[<span class="dv">2</span>] +
         dcm[<span class="dv">5</span>] * (<span class="dv">1</span> - phi[<span class="dv">0</span>] * phi[<span class="dv">1</span>] * phi[<span class="dv">2</span>]) +
         dcm[<span class="dv">8</span>] * (phi[<span class="dv">0</span>] + phi[<span class="dv">1</span>] * phi[<span class="dv">2</span>]);
dcm[<span class="dv">6</span>] = dcm[<span class="dv">0</span>] * phi[<span class="dv">1</span>] + dcm[<span class="dv">3</span>] * -phi[<span class="dv">0</span>] + dcm[<span class="dv">6</span>];
dcm[<span class="dv">7</span>] = dcm[<span class="dv">1</span>] * phi[<span class="dv">1</span>] + dcm[<span class="dv">4</span>] * -phi[<span class="dv">0</span>] + dcm[<span class="dv">7</span>];
dcm[<span class="dv">8</span>] = dcm[<span class="dv">2</span>] * phi[<span class="dv">1</span>] + dcm[<span class="dv">5</span>] * -phi[<span class="dv">0</span>] + dcm[<span class="dv">8</span>];</code></pre>
<p>where <code>phi</code> is the angular displacement angle.</p>
<h2 id="satellite-navigation-subsystem"><a href="#satellite-navigation-subsystem">Satellite navigation subsystem</a></h2>
<h3 id="communication"><a href="#communication">Communication</a></h3>
<p>Systems such GPS uses numerous satellites orbiting Earth transmitting their position and time. Receiver can measure its position and time with accuracy based on number of satellites in view. GPS receivers usually communicate via serial line, Linux kernel features TTY module originally used for teletypewriters. It handles serial lines, converters and virtual lines with devices generally called <code>/dev/tty*</code>. Serial connections (UART, RS-232) are usually named <code>/dev/ttySn</code>, where <code>n</code> is a device number. Emulated connection are named <code>/dev/ttyUSBn</code> for USB emulators or <code>/dev/ttyACMn</code> for modem emulators. There are also pseudo terminals in <code>/dev/pts/</code> used for software emulation. This devices allows standard I/O and may be controlled with functions defined in <code>termios.h</code> or by using <a href="http://linux.die.net/man/1/stty">stty(1)</a> utility. Important physical interface settings are</p>
<ul>
<li>baud rate</li>
<li>parity</li>
<li>flow control</li>
</ul>
<p>Usual settings are 4800 or 38 400 baud with no parity and no flow control, without proper interface configuration, no communication will occur Driver further provides line processing before passing data to the application. Important line settings are</p>
<ul>
<li>newline handling</li>
<li>echo mode</li>
<li>canonical mode</li>
<li>timeouts and buffer sizes</li>
</ul>
<p>In canonical mode, driver uses line buffer and allows line editing. Buffer is passed to the application (made readable) after new line is encountered. It may be combined with echo mode, which will automatically send received characters back. This is how standard console line works, however if application needs to be in full control, driver may be switched to raw mode with per-character buffers and echo disabled. Also specific control characters for canonical mode may be configured.</p>
<p>GPS receivers uses <a href="http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp">NMEA 183 standard</a> <span class="citation">[11]</span>. Communication is done in sentences, each message starts with dollar sign followed by source name and sentence type, then there is comma separated list of data fields optionally terminated by a checksum. Each sentence is appended with carriage return and new line. For example in sentence</p>
<p><code class="sourceCode perl"><span class="dt">$GPGGA</span>,<span class="dv">000000</span>,<span class="fl">4914.266</span>,N,<span class="fl">01638.583</span>,E,<span class="dv">1</span>,<span class="dv">8</span>,<span class="dv">1</span>,<span class="dv">257</span>,M,<span class="fl">46.43</span>,M,,*<span class="dv">46</span></code></p>
<p><code>GP</code> is source name (GPS receiver), <code>GGA</code> is sentence type (fix information), <code>*46</code> is a checksum and rest are data fields. Checksum is delimited by asterisk and consist of two character hexadecimal number calculated as bitwise XOR of all character between dollar and asterisk. Note that fields may be empty (as last two fields in the example) and numbers may have fractional parts.</p>
<table>
<caption>Important NMEA 0183 sentences</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
<th align="left">Important information</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">GSA</td>
<td align="left">Satellites (overall)</td>
<td align="left">Visible satellites, dilution of precision</td>
</tr>
<tr class="even">
<td align="left">GSV</td>
<td align="left">Satellites (detailed)</td>
<td align="left">Satellite number, elevation, azimuth (per satellite)</td>
</tr>
<tr class="odd">
<td align="left">GGA</td>
<td align="left">Fix information</td>
<td align="left">Time, latitude, longitude, altitude, fix quality</td>
</tr>
<tr class="even">
<td align="left">RMC</td>
<td align="left">Position data</td>
<td align="left">Time, latitude, longitude, speed, track</td>
</tr>
<tr class="odd">
<td align="left">RMB</td>
<td align="left">Navigation data</td>
<td align="left">Destination, range, bearing</td>
</tr>
<tr class="even">
<td align="left">WPL</td>
<td align="left">Waypoint data</td>
<td align="left">Latitude, longitude, name</td>
</tr>
</tbody>
</table>
<p>The <strong>GSA</strong> sentence has following fields:</p>
<ul>
<li>1st field is <code>A</code> for automatic selection of 2D or 3D fix, <code>M</code> for manual</li>
<li>2nd field is <code>3</code> for 3D fix, <code>2</code> for 2D fix or <code>1</code> for no fix</li>
<li>3-14 fields are identification numbers (PRN) of satellites in view</li>
<li>15-17 fields are dilution of precision and its horizontal and vertical parts</li>
</ul>
<p>The <strong>GSV</strong> sentence has following fields:</p>
<ul>
<li>1-2 fields are number of partial sentences and number of current part</li>
<li>3rd field is number of satellites in view</li>
<li>4th field is satellite number (PRN)</li>
<li>5th field is satellite elevation</li>
<li>6th field is satellite azimuth</li>
<li>7th field is satellite signal to noise ratio in decibels</li>
<li>8-11, 12-15, 16-19 fields are for other satellites info (number, elevation, azimuth, SNR)</li>
</ul>
<p>The <strong>GGA</strong> sentence has following fields:</p>
<ul>
<li>1st field resembles time of the fix in format <code>hhmmss.ff</code>, where <code>hh</code> are hours, <code>mm</code> are minutes, <code>ss</code> are seconds and <code>ff</code> are fractional seconds.</li>
<li>2-3 fields resembles device latitude in format <code>ddmm.ff,[NS]</code>, where <code>dd</code> are degrees, <code>mm</code> are minutes, <code>ff</code> are fractional minutes, <code>N</code> means north hemisphere and <code>S</code> means south.</li>
<li>4-5 fields resembles device longitude in format <code>dddmm.ff,[EW]</code>, where <code>E</code> means east of Greenwich and <code>W</code> means west.</li>
<li>6th field is fix type, 1 means GPS fix</li>
<li>7th field is number of satellites in view</li>
<li>8th field is horizontal dilution of precision</li>
<li>9-10 fields resembles device altitude above mean sea level, first field is the value and second field is the unit used</li>
<li>11-12 fields resembles height of geoid above WGS84 in the same fashion</li>
<li>13-14 fields are usually unused and refers to differential GPS</li>
</ul>
<p>The <strong>RMC</strong> sentence has following fields:</p>
<ul>
<li>1-5 fields are same as in GGA sentence</li>
<li>6th field is speed over the ground in knots</li>
<li>7th field is track angle in degrees</li>
<li>8th field resembles date in format <code>ddmmyy</code>, where <code>dd</code> is day, <code>mm</code> is month and <code>yy</code> is last two digits of year</li>
<li>9-10 fields resembles magnetic variation, first field is value in degrees, second field is <code>E</code> meaning east or <code>W</code> meaning west</li>
</ul>
<p>The <strong>RMB</strong> sentence has following fields:</p>
<ul>
<li>1st field is status, <code>A</code> means OK</li>
<li>2-3 fields resembles cross-track error, first field is the value in nautical miles, second field is <code>E</code> meaning east or <code>W</code> meaning west</li>
<li>4th field is origin waypoint name</li>
<li>5th field is destination waypoint name</li>
<li>6-9 fields are destination waypoint latitude and longitude with the same formatting as in the GGA sentence</li>
<li>10th field is range to destination in nautical miles</li>
<li>11th field is bearing to destination in degrees</li>
<li>12th field is velocity towards destination in knots</li>
<li>13th field is <code>A</code> for arrived or <code>V</code> for not arrived to destination</li>
</ul>
<p>The <strong>WPL</strong> sentence has following fields:</p>
<ul>
<li>1-2 fields are waypoint coordinates with the same formatting as in the GGA sentence</li>
<li>3rd field is a waypoint identifier string</li>
</ul>
<p>Many navigation systems use proprietary sentences, they begin with the <em>P</em> prefix. For example some Garmin products specific sentences <span class="citation">[12]</span> are listed in the following table.</p>
<table>
<caption>Garmin proprietary sentences</caption>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Description</th>
<th align="left">Important information</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">PGRME</td>
<td align="left">Estimated error</td>
<td align="left">Horizontal, vertical position error</td>
</tr>
<tr class="even">
<td align="left">PGRMF</td>
<td align="left">Fix data</td>
<td align="left">Date, time, latitude, longitude, speed, course</td>
</tr>
<tr class="odd">
<td align="left">PGRMH</td>
<td align="left">VNAV data</td>
<td align="left">Vertical speed, vertical speed to waypoint, height above terrain</td>
</tr>
<tr class="even">
<td align="left">PGRMT</td>
<td align="left">Sensor status</td>
<td align="left">State information, ambient temperature</td>
</tr>
<tr class="odd">
<td align="left">PGRMV</td>
<td align="left">3D velocity vector</td>
<td align="left">North, east, up velocity</td>
</tr>
<tr class="even">
<td align="left">PGRMZ</td>
<td align="left">Altitude</td>
<td align="left">Altitude</td>
</tr>
</tbody>
</table>
<p>Sending data to the device is also possible, there are two useful sentences. The PGRMC sentence configures the device including the baud-rate, the PGRMO sentence enables / disables specific sentences. This is important as the data rate must fit available bandwidth and therefore limiting the sentence interval.</p>
<p></p>
<p>Typical default baudrate is only 4800, the table bellow shows approximate data rates when the baudrate is increased.</p>
<table>
<caption>NMEA 0183 data throughput</caption>
<thead>
<tr class="header">
<th align="right">Baudrate</th>
<th align="left">Sentences enabled</th>
<th align="right">Max length</th>
<th align="right">Records per second</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4800</td>
<td align="left">GPGGA, GPRMB, GPRMC</td>
<td align="right">180</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">4800</td>
<td align="left">PGRME, PGRMF, PGRMT</td>
<td align="right">167</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">4800</td>
<td align="left">GPGSA, GPGSV, GPGGA, GPRMB, GPRMC</td>
<td align="right">310</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="right">9600</td>
<td align="left">GPGSA, GPGSV, GPGGA, GPRMB, GPRMC</td>
<td align="right">310</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">9600</td>
<td align="left">PGRME, PGRMF, PGRMT</td>
<td align="right">167</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="right">19200</td>
<td align="left">GPGSA, GPGSV, GPGGA, GPRMB, GPRMC</td>
<td align="right">310</td>
<td align="right">6</td>
</tr>
</tbody>
</table>
<h3 id="navigation"><a href="#navigation">Navigation</a></h3>
<p>As Earth shape is very complex, there are two layers of approximation used for computing position. Geoid is the equipotential surface, which describes mean ocean level if Earth was fully covered with water. Most recent geoid model is EGM96 which is used together with <a href="http://earth-info.nga.mil/GandG/publications/tr8350.2/wgs84fin.pdf">WGS84 reference ellipsoid</a> <span class="citation">[13]</span>. This ellipsoid has semi-major axis of <span class="math">\(a = 6378137\)</span> meters and flattening <span class="math">\(f = 1/298.257223563\)</span>. Note that ellipsoid flattening is defined as</p>
<p><span class="math">\(f = \dfrac{a - b}{a}\)</span>,</p>
<p>where <em>b</em> is the semi-minor axis. The eccentricity is defined as</p>
<p><span class="math">\(e = 2f - f^2\)</span>.</p>
<p>Geodetic latitude <span class="math">\(\varphi\)</span> is the angle between normal to the reference ellipsoid and the equator, longitude <span class="math">\(\lambda\)</span> is the angle between normal to the reference ellipsoid and the prime meridian. Because of the flattening, the normal does not intersect ellipsoid center. Geocentric latitude <span class="math">\(\psi\)</span> uses line running through the center instead of the normal,</p>
<p><span class="math">\(\psi = \arctan [ (1-e)^2 \tan(\varphi)]\)</span>.</p>
<p>Device position measured by GPS is defined by its geodetic latitude, longitude and altitude</p>
<p><span class="math">\(h_{AMSL} = h_{WGS84} - h_{EGM96}\)</span></p>
<p>measured as height above mean see level, where <span class="math">\(h_{WGS84}\)</span> is the height above reference ellipsoid and <span class="math">\(h_{EGM96}\)</span> is the height above geoid. GPS sensors usually send fix information at low rates and with high noise. Position needs to be interpolated and filtered between fixes. To improve precision and especially dynamic response position information may be augmented with inertial measurements. The current speed vector can be calculated as</p>
<p><span class="math">\(\overrightarrow{v}_{(t)} = W_{SAT} \begin{bmatrix} v_{GND} \cos(\alpha_{TRK}) \\ v_{GND} \sin(\alpha_{TRK}) \\ h_{baro(t)} - h_{baro(t-_\Delta t)} \end{bmatrix} + (1-W_{SAT}) \overrightarrow{v}_{(t-_\Delta t)} + \mathbf{DCM} \times \overrightarrow{\mathbf{a}}_{acc} \cdot _\Delta t\)</span>,</p>
<p>where <span class="math">\(W_{SAT}\)</span> is the weight of the satellite measurement, <span class="math">\(v_{GND}\)</span> and <span class="math">\(\alpha_{TRK}\)</span> are speed and track angle from RMC sentence, <span class="math">\(h_{baro}\)</span> and <span class="math">\(\mathbf{a}_{acc}\)</span> are inertial measurements. The interpolated position in the current step will be</p>
<p><span class="math">\(\varphi_{(t)} = W_{SAT} \cdot \varphi_{SAT} + (1-W_{SAT}) \cdot \varphi_{(t-_\Delta t)} + \dfrac{v_{x(t)} \cdot _\Delta t}{R_{(\varphi)}}\)</span>,</p>
<p><span class="math">\(\lambda_{(t)} = W_{SAT} \cdot \lambda_{SAT} + (1-W_{SAT}) \cdot \lambda_{(t-_\Delta t)} + \dfrac{v_{y(t)} \cdot _\Delta t}{R_{(\varphi)} \cdot \cos(\varphi)}\)</span>,</p>
<p><span class="math">\(h_{(t)} = W_{SAT} \cdot h_{SAT} + (1-W_{SAT}) \cdot h_{(t-_\Delta t)} + v_{z(t)} \cdot _\Delta t\)</span>,</p>
<p>where <em>R</em> is the ellipsoid radius at the given latitude</p>
<p><span class="math">\(R = \dfrac{\sqrt{b^4 \sin(\varphi)^2 + a^4 \cos(\varphi)^2}}{\sqrt{b^2 \sin(\varphi)^2 + a^2 \cos(\varphi)^2}}\)</span>.</p>
<p>Application needs to know projections of specific landmarks as normalized horizontal and vertical coordinates (in range of -1 to 1) used for rendering</p>
<p><span class="math">\(\begin{bmatrix} x \cdot FOV_x \\ y \cdot FOV_y \end{bmatrix} =  \begin{bmatrix} \cos(\gamma_{dev}) &amp; -\sin(\gamma_{dev}) \\ \sin(\gamma_{dev}) &amp; \cos(\gamma_{dev})  \end{bmatrix} \times \begin{bmatrix} \alpha_{proj} - \alpha_{dev} \\ \beta_{proj} - \beta_{dev} \end{bmatrix} \)</span>,</p>
<p>where <em>FOV</em> is the field of view, <em>dev</em> means device angle (calculated by <a href="#inertial-measurement-subsystem">inertial subsystem (2.4)</a>) and <em>proj</em> means projection angle (defined later on in this section). Orthodrome (great circle) is the intersection of a sphere and a plane passing though its center. However, because Earth flattening is rather small, it may be used as an approximation for a curve following Earth surface, connecting two points with shortest route. <a href="http://mathworld.wolfram.com/SphericalTrigonometry.html">Spherical trigonometry</a> <span class="citation">[14]</span> defines basis for orthodrome calculations, shown in the illustration below.</p>
<div class="figure">
<img src="images/hangle.svg" alt="Horizontal projection angle" /><p class="caption">Horizontal projection angle</p>
</div>
<p>Heading changes along the route and its initial value is the horizontal projection angle</p>
<p><span class="math">\(\alpha_{proj} = \arctan \left ( \dfrac{\sin(\lambda - \lambda_0) \cos(\varphi)} {\cos(\varphi_0) \sin(\varphi) - \sin(\varphi_0) \cos(\varphi) \cos(\lambda - \lambda_0)} \right )\)</span>,</p>
<p>the zero index refers to the device coordinate. Angular distance between those two points is</p>
<p><span class="math">\(\phi = \arccos(\sin(\varphi_0) \sin(\varphi) + \cos(\varphi_0) \cos(\varphi) \cos(\lambda - \lambda_0))\)</span>.</p>
<p>Vertical projection angle is the angle between the horizon (perpendicular to normal) and a line directly connecting the points. Lets construct a triangle connecting the points with the center of the reference ellipsoid as in the illustration below. Zero flattening is assumed, so the normals have intersection in the center (<span class="math">\(f = 0 \rightarrow e = 0 \rightarrow \psi = \varphi\)</span>).</p>
<div class="figure">
<img src="images/vangle.svg" alt="Vertical projection angle" /><p class="caption">Vertical projection angle</p>
</div>
<p>The triangle sides are</p>
<p><span class="math">\(a_\Delta = h_0 + R_{(\varphi_0)}\)</span>,</p>
<p><span class="math">\(b_\Delta = h + R_{(\varphi)}\)</span>,</p>
<p><span class="math">\(c_\Delta = \sqrt{a_\Delta^2 + b_\Delta^2 - 2 a_\Delta b_\Delta \cos(\phi)}\)</span>,</p>
<p><span class="math">\(\beta_{proj} = \arcsin \left ( \dfrac{b_\Delta}{c_\Delta} \sin(\phi) \right ) - \frac{\pi}{2}\)</span>,</p>
<p>where <em>h</em> is height above reference ellipsoid. These calculations are quite complex and there is plenty of room for approximation. Over short distance, such as typical in visual ranges, orthodromes may be replaced with loxodromes (rhumb lines). Loxodrome is a curve following Earth surface, connecting two points with constant heading. It has similar path to orthodrome, if the points are relatively far from poles a close together. Simplified heading along the line is</p>
<p><span class="math">\(\alpha_{proj} \doteq \arctan \left ( \dfrac{\lambda \cos(\varphi) - \lambda_0 \cos(\varphi_0)}{\varphi - \varphi_0} \right )\)</span>.</p>
<p>Note that <a href="http://www.progonos.com/furuti/MapProj/Normal/CartProp/Rhumb/rhumb.html">loxodrome approximation</a> will fail horribly near poles as the curve will run circles around the pole <span class="citation">[15]</span>. Angular distance along loxodrome is</p>
<p><span class="math">\(\phi \doteq \sqrt{(\varphi - \varphi_0)^2 + (\lambda \cos(\varphi) - \lambda_0 \cos(\varphi_0))^2}\)</span></p>
<p>and a horizontal distance along the arc between those points is</p>
<p><span class="math">\(d = \phi \cdot R_{(\varphi_0)}\)</span>,</p>
<p>assuming flattening difference is close to zero, so <em>R</em> is constant along the curve. With the approximation of local flat Earth surface perpendicular to the normal, the simplified vertical projection angle is</p>
<p><span class="math">\(\beta_{proj} \doteq \arctan \left ( \dfrac{h - h_0}{d} \right )\)</span>.</p>
<p>This approximation will fail at higher altitudes when the visibility range is high enough to make the Earth curvature observable.</p>
<h3 id="elevation-mapping"><a href="#elevation-mapping">Elevation mapping</a></h3>
<p>Waypoints are usually defined only by latitude and longitude, as their altitude equals terrain altitude (for example the WPL sentence). To determine the terrain topology, elevation map may be used. This is a scalar field usually encapsulated into raster image with meta-data. <a href="http://www.gisdevelopment.net/technology/ip/mi03117pf.htm">GeoTIFF</a> <span class="citation">[16]</span> is a standardized format defining georeferencing information within a TIFF file. Digital elevation models in this format are available for example at <a href="http://ned.usgs.gov/index.html">USGS</a> (United States) or <a href="http://epp.eurostat.ec.europa.eu/portal/page/portal/gisco_Geographical_information_maps/geodata/digital_elevation_model">Eurostat</a> (Europe). There are several tools for working with this format, for example the <a href="http://fwtools.maptools.org/">FWTools</a> open source GIS binary kit. To export quantized pixel map from the elevation model, the toolkit comes with the <em>gdal_translate</em> utility. The following command will generate PNG image of the South Moravian Region of the Czech Republic</p>
<pre><code>gdal_translate eudem_dem_5deg_n45e015.tif dem48_50n15_18e.png \
 -srcwin 0 0 10800 7200 -ot UInt16 -of PNG -scale 0 1000 0 65535</code></pre>
<p>This file can be read by the <em>libpng</em> library</p>
<pre class="sourceCode c"><code class="sourceCode c">FILE *f = fopen(<span class="st">&quot;dem48_50n15_18e.png&quot;</span>, <span class="st">&quot;rb&quot;</span>);
png_structp png = png_create_read_struct(PNG_LIBPNG_VER_STRING, NULL, NULL, NULL);
png_infop info = png_create_info_struct(png);
png_init_io(png, f);
png_read_info(png, info);
size_t num = png_get_image_height(png, info);
size_t len = png_get_rowbytes(png, info);
png_bytep *p = malloc(num * <span class="kw">sizeof</span>(png_bytep));
<span class="dt">int</span> i; <span class="kw">for</span>(i = <span class="dv">0</span>; i &lt; num; i++) p[i] = malloc(len);
png_read_image(png, p);</code></pre>
<!---->

<p>Then to calculate the actual elevation at the given coordinates</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="ot">#define LEFT   15    </span><span class="co">// degrees east</span>
<span class="ot">#define RIGHT  18    </span><span class="co">// degrees east</span>
<span class="ot">#define TOP    50    </span><span class="co">// degrees north</span>
<span class="ot">#define BOTTOM 48    </span><span class="co">// degrees north</span>
<span class="ot">#define WIDTH  10800 </span><span class="co">// pixels</span>
<span class="ot">#define HEIGHT 7200  </span><span class="co">// pixels</span>
<span class="ot">#define SCALE  1000  </span><span class="co">// meters per 0xFFFF</span>
<span class="dt">int</span> x = (lon - LEFT) / (RIGHT - LEFT) * WIDTH + <span class="fl">0.5</span>;
<span class="dt">int</span> y = (TOP - lat) / (TOP - BOTTOM) * HEIGHT + <span class="fl">0.5</span>;
<span class="dt">double</span> value = (<span class="dt">double</span>)(((<span class="dt">uint16_t</span>)p[y][x * <span class="dv">2</span>] &lt;&lt; <span class="dv">8</span>) |
                         (<span class="dt">uint16_t</span>)p[y][x * <span class="dv">2</span> + <span class="dv">1</span>]) / <span class="bn">0xFFFF</span> * SCALE;</code></pre>
<!---->

<p>This value should be bi-linearly interpolated across the four neighboring pixels located at the nearest integer values <em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, <em>y<sub>1</sub></em>, <em>y<sub>2</sub></em>. The intermediate linear interpolation is calculated as</p>
<p><span class="math">\(f_{(x,y_1)} = \dfrac{x_2 - x}{x_2 - x_1} f_{(x_1,y_1)} + \dfrac{x - x_1}{x_2 - x_1} f_{(x_2,y_1)}\)</span>,</p>
<p><span class="math">\(f_{(x,y_2)} = \dfrac{x_2 - x}{x_2 - x_1} f_{(x_1,y_2)} + \dfrac{x - x_1}{x_2 - x_1} f_{(x_2,y_2)}\)</span>.</p>
<p>The final value would then be</p>
<p><span class="math">\(f(x,y) = \dfrac{y_2 - y}{y_2 - y_1} f_{(x,y_1)} + \dfrac{y - y_1}{y_2 - y_1} f_{(x,y_2)}\)</span>.</p>
<h2 id="output-creation"><a href="#output-creation">Output creation</a></h2>
<p>The image below shows the actual output rendered by the application, there is no video source (solid white background). Resolution was reduced to 320x240 to make the image small, this caused some aliasing artifacts normally not visible. In the top left corner is the current speed over ground as reported by the satellite navigation, while in the top right corner is the current altitude above mean sea level. Speed is displayed in kilometers per hour and the altitude in meters. In the top center is the current waypoint name and distance in kilometers, if provided by the NMEA 0183 navigation data sentence (RMB). There is a heading ruler in the bottom, it is scaled by camera field of view and oriented to current heading. Course to any visible object may be deducted directly from the ruler. There are two markers, the arrow marker point to current track angle as reported by the satellite navigation. The circle marker is the course to current waypoint if any. The dashed horizontal line in the center of the image is the horizon line, it follows the horizon as calculated by inertial subsystem by moving up or down and rotating around its center. Orientation is determined by the marginal markers, they points always to the ground. If the horizon is not visible, dashed arrow is shown instead pointing up or down to where the horizon is hidden.</p>
<p></p>
<div class="figure">
<img src="images/output.png" alt="Visible output" /><p class="caption">Visible output</p>
</div>
<p>Application accepts database of landmarks, following file was used in this example:</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Encoding ISO/IEC 8859-1</span>
<span class="co"># Test landmarks (lat[rad], lon[rad], alt[m], name)</span>
<span class="fl">25e-5</span>, <span class="fl">1e-5</span>, <span class="dv">0</span>, testA
<span class="fl">50e-5</span>, -<span class="fl">1e-5</span>, <span class="dv">100</span>, testB</code></pre>
<p>The <code>testA</code> landmark label is visible in the image, its location is centered directly above its projection. Projections are calculated in conjunction of both navigation subsystems and graphic acceleration. Large landmark database may be supplemented to provide spatial navigation references, only visible landmarks will be shown. This overlay is rendered in real time over the source video (there is none in the example), all rendering and data gathering methods are provided by the respective subsystems. Upon correct configuration, overlay elements (ruler, horizon and labels) should be aligned with the real visible places in the video frame. System allows free six degrees of freedom movement of the camera while still being able to render the overlay correctly. Waypoint management and route planning is done solely by the external navigation system, it is expected that it will provide its own user interface. This allows connection for example to PDAs or other specialized devices which delivers classic 2D moving map navigation to the user with its own controls.</p>
<h1 id="hardware"><a href="#hardware">Hardware</a></h1>
<div class="figure">
<img src="images/hardware.svg" alt="Proposed hardware solution" /><p class="caption">Proposed hardware solution</p>
</div>
<p>The diagram above specifies the proposed platform realization. The Texas Instruments <a href="http://www.ti.com/product/omap4460">OMAP4460</a> <span class="citation">[17]</span> application processor was chosen for the project, however the portable nature of the application does not make this a requirement. For example the AM335x family of processors was tested and works as well.</p>
<p>To provide flexible power supply a specialized chip such as the TWL6032 power companion for the OMAP platform should be used to</p>
<ul>
<li>DC-DC voltage conversion and power routing</li>
<li>battery supply, battery charging</li>
<li>automatic switch between external power and battery</li>
<li>USB power with maximum current negotiation and limiting</li>
</ul>
<p>Total power consumption should be below 3 Watts, depending on peripherals (without display). The power solution should seamlessly switch between power sources to provide enough power and use external supply whenever possible.</p>
<p>The OMAP4 platform features two SMP ARM Cortex-A9 general-purpose CPUs with NEON vector floating point accelerator. Application makes use of the SMP to distribute processing power between its subsystem threads. The NEON SIMD extensions allows efficient implementation of the JPEG compression algorithm, the performance scales up to the full HD resolution. For more complex coders such as the MPEG4 AVC / H.264, there is a specialized IVA subsystem embedded in the OMAP4 platform. It consists of two Cortex-M3 cores for real-time computations and a video accelerator unit, the subsystem is capable of simultaneous real-time encoding and decoding of the video at full HD resolution.</p>
<p>Application has a flexible input video support, there are many possible solution for hardware implementation as virtually any device supported by the Linux kernel should work. The most straightforward implementation would be direct connection to the camera chip, for this purpose there are two CSI-2 interfaces on the OMAP4 platform as a port of the embedded imaging subsystem (ISS).</p>
<p>The <a href="http://mipi.org/specifications/camera-interface">MIPI CSI-2</a> <span class="citation">[18]</span> interface is the standard serial camera interface, consisting of clock and pixel data differential lines as shown in the diagram below.</p>
<div class="figure">
<img src="images/csi.svg" alt="CSI interface" /><p class="caption">CSI interface</p>
</div>
<p>The pixel data are transmitted synchronously, there is also usually an I2C control interface. The ISS on the OMAP side features an image signal processor capable of auto focus, auto exposure, and auto white balance computations. The data throughput of the subsystem scales up to 200 MPix/s. There are many supported image sensors such as the Omnivision <a href="http://www.ovt.com/products/sensor.php?id=93">OV5640</a>, it has 5MP resolution with 1080p RGB output at 30 FPS.</p>
<p>External video sources are also supported, they may connect by either USB or Ethernet USB Video Class (UVC) driver is a part of the Linux kernel V4L2 module and works with almost any UVC device, USB devices are self descriptive. USB ports does not have enough throughput for raw video at HD resolutions, so most cameras usually uses MJPEG compression, USB cable length is also limited to few meters. UVC devices features their own image processors and are configured via the USB interface by the UVC driver.</p>
<p>Ethernet connection is the most flexible solution, however also the most complicated, it is supported only by high end cameras and does not have standardized control interface. Control is usually provided via a micro HTTP server on the camera side, video is usually streamed encapsulated in Real-time Transport Protocol (RTP) packets over the UDP socket. The typical encapsulation process consists of MPEG4 AVC / H.264 encoder, RTP payloader and UDP / IP layers. On the receiver side there is the RTP jitter buffer, RTP de-payloader and AVC decoder. The best way to implement this pipeline is to use the GStreamer utility, it also supports the IVA accelerator for decoding. On the physical layer there is no direct interface on the OMAP4 platform, either MAC-PHY or WiFi chips must be used. Using the WiFi radio is probably the most sophisticated way of video input as it leaves the system physically isolated from the video input device.</p>
<p>For graphic acceleration the application fully depends on the embedded GPU, which is usually platform dependent. There is the PoverVR SGX540 chip integrated within the OMAP4 platform supporting the OpenGL ES 2.0 framework, any other accelerator with the same OpenGL framework and with kernel support will work. The video is fed to the display sub-system which features an overlay manager and an integrated HDMI v1.3 output compatible with most modern displays. The video output may also be streamed via Ethernet in a similar way as the video input, however this is again a more complex solution.</p>
<p>The navigation peripherals consists of the serial interface for the NMEA 0183 protocol and the I2C interface for the IIO module. Serial connection may be done via UART port with voltage level converter such as MAX232 which provides RS-232 compatible interface. It consists of duplex asynchronous receive and transmit lines, and the ground line as shown in the diagram bellow.</p>
<div class="figure">
<img src="images/uart.svg" alt="UART interface" /><p class="caption">UART interface</p>
</div>
<p>Another option is the use of the USB Comunication Device Class (USB-CDC) driver which will bridge the serial line over the USB, using UDP sockets over the Ethernet is also a possibility.</p>
<p>The inertial sensors are connected via the I<sup>2</sup>C bus, it is a master-slave synchronous bus consisting of clock and data lines, and the ground line. The I<sup>2</sup>C protocol allows multiple devices to be interconnected, it supports addressing and arbitration. The communication is simplex and is controlled and clocked by the master, each slave have a fixed address to which it responds as shown in the diagram below.</p>
<div class="figure">
<img src="images/i2c.svg" alt="I2C interface" /><p class="caption">I<sup>2</sup>C interface</p>
</div>
<p>The I<sup>2</sup>C interface is typically used to directly read or write registers of the interconnected devices, each device must have a device driver in the Linux kernel for abstraction.</p>
<p>The Invensense <a href="http://www.invensense.com/mems/gyro/mpu9150.html">MPU-9150</a> <span class="citation">[19]</span> is an all-in-one motion tracking device composed of an embedded MPU-6050 3-axis gyroscope and accelerometer and a AK8975 3-axis digital compass. Many other individual devices are supported by the kernel IIO tree, such as Freescale MPL3115 barometer, Invensense ITG-3200 gyroscope or BMA180 accelerometer.</p>
<p>There are many development boards already available for generic testing. The <a href="http://pandaboard.org/">Pandaboard ES</a> <span class="citation">[20]</span> features the OMAP4460 and provides all peripheral interfaces. There is also a low price board the <a href="http://beagleboard.org/products/beaglebone%20black">BeagleBone Black</a> <span class="citation">[21]</span> with a Sitara AM3358 processor. Texas Instruments also distributes the <a href="http://www.ti.com/tool/boostxl-senshub">Sensor Hub BoosterPack</a> <span class="citation">[22]</span> with all needed sensors.</p>
<p>Application may also be fully emulated on the PC in the Linux environment. OpenGL ES 2.0 should run natively under Mesa3D library. For video interface, GStreamer may be used together with the <em>v4l2loopback</em> module. Though serial interfaces needs to be emulated manually.</p>
<h1 id="conclusion"><a href="#conclusion">Conclusion</a></h1>
<p>The application has been successfully implemented, its complete source code is available under the GPL license at <span class="citation">[23]</span>. As a prototype, the Pandaboard development board was chosen for realization. The complete system is highly modular and configurable, the primary video source is either USB or WiFi connected camera and a 7-inch high contrast HDMI display panel is used for the output. Both MJPEG and H.264 coders are functional as well as raw RGB or YUV formats. The application is limited to 720p resolution because of the display, however it is fully capable of generating full HD video. The navigation subsystems integrate well with existing external devices, the combination of satellite and inertial sensors results in precise positional and spatial information. Complex filtering algorithm provides stable attitude necessary to avoid screen jittering, projections are also interpolated over time. No sophisticated user interface was implemented as the application can share most data (waypoints, enroute navigation) with the existing systems via its serial connection and those systems already have complex user interfaces. This means the waypoint management is centralized and not duplicated by the application. The final device is deployable into an augmented flight navigation system.</p>
<h1 id="threads-example"><a href="#threads-example">Threads example</a></h1>
<p>In this example two threads share standard input and output, access is restricted by <em>mutex</em> so only one thread may use the shared resource at any time.</p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="sourceCode"><pre><code class="sourceCode c"><span class="ot">#include &lt;stdio.h&gt;</span>
<span class="ot">#include &lt;pthread.h&gt;</span>

<span class="dt">void</span> *worker(<span class="dt">void</span> *arg)
{
    <span class="dt">static</span> <span class="dt">int</span> counter = <span class="dv">1</span>;
    pthread_mutex_t *mutex = (pthread_mutex_t*)arg;
    <span class="dt">char</span> *buffer;

    <span class="co">// Lock mutex to restrict access to stdin and stdout</span>
    pthread_mutex_lock(mutex);
    printf(<span class="st">&quot;This is worker %d, enter something: &quot;</span>, counter++);
    scanf(<span class="st">&quot;%ms&quot;</span>, &amp;buffer);
    pthread_mutex_unlock(mutex);

    <span class="kw">return</span> (<span class="dt">void</span>*)buffer;
}

<span class="dt">int</span> main()
{
    pthread_mutex_t mutex;
    pthread_t thread1, thread2;
    <span class="dt">char</span> *retval1, *retval2;

    <span class="co">// Initialize two threads with shared mutex, use default parameters</span>
    pthread_mutex_init(&amp;mutex, NULL);
    pthread_create(&amp;thread1, NULL, worker, (<span class="dt">void</span>*)&amp;mutex);
    pthread_create(&amp;thread2, NULL, worker, (<span class="dt">void</span>*)&amp;mutex);

    <span class="co">// Wait for both threads to finish and display results</span>
    pthread_join(thread1, (<span class="dt">void</span>**)&amp;retval1);
    pthread_join(thread2, (<span class="dt">void</span>**)&amp;retval2);
    printf(<span class="st">&quot;Thread 1 returned with `%s`.</span><span class="ch">\n</span><span class="st">&quot;</span>, retval1);
    printf(<span class="st">&quot;Thread 2 returned with `%s`.</span><span class="ch">\n</span><span class="st">&quot;</span>, retval2);

    pthread_mutex_destroy(&amp;mutex);
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre></td></tr></table>
<h1 id="video-capture-example"><a href="#video-capture-example">Video capture example</a></h1>
<p>In this example video device is configured to capture frames using memory mapping. These frames are dumped to standard output, instead of further processing.</p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
</pre></td><td class="sourceCode"><pre><code class="sourceCode c"><span class="ot">#include &lt;fcntl.h&gt;</span>
<span class="ot">#include &lt;unistd.h&gt;</span>
<span class="ot">#include &lt;poll.h&gt;</span>
<span class="ot">#include &lt;sys/mman.h&gt;</span>
<span class="ot">#include &lt;sys/ioctl.h&gt;</span>
<span class="ot">#include &lt;linux/videodev2.h&gt;</span>

<span class="dt">int</span> main()
{
    <span class="co">// Open device</span>
    <span class="dt">int</span> fd = open(<span class="st">&quot;/dev/video0&quot;</span>, O_RDWR | O_NONBLOCK);

    <span class="co">// Set video format</span>
    <span class="kw">struct</span> v4l2_format format =
    {
        .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
        .fmt =
        {
            .pix =
            {
                .width = <span class="dv">320</span>,
                .height = <span class="dv">240</span>,
                .pixelformat = V4L2_PIX_FMT_RGB32,
                .field = V4L2_FIELD_NONE,
                .colorspace = V4L2_COLORSPACE_SMPTE170M,
            },
        },
    };
    ioctl(fd, VIDIOC_S_FMT, &amp;format);

    <span class="co">// Request buffers</span>
    <span class="kw">struct</span> v4l2_requestbuffers requestbuffers =
    {
        .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
        .memory = V4L2_MEMORY_MMAP,
        .count = <span class="dv">4</span>,
    };
    ioctl(fd, VIDIOC_REQBUFS, &amp;requestbuffers);
    <span class="dt">void</span> *pbuffers[requestbuffers.count];

    <span class="co">// Map and enqueue buffers</span>
    <span class="dt">int</span> i;
    <span class="kw">for</span>(i = <span class="dv">0</span>; i &lt; requestbuffers.count; i++)
    {
        <span class="kw">struct</span> v4l2_buffer buffer = 
        {
            .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
            .memory = V4L2_MEMORY_MMAP,
            .index = i,
        };
        ioctl(fd, VIDIOC_QUERYBUF, &amp;buffer);
        pbuffers[i] = mmap(NULL, buffer.length,
                           PROT_READ | PROT_WRITE, MAP_SHARED,
                           fd, buffer.m.offset);
        ioctl(fd, VIDIOC_QBUF, &amp;buffer);
    }

    <span class="co">// Start stream</span>
    <span class="kw">enum</span> v4l2_buf_type buf_type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    ioctl(fd, VIDIOC_STREAMON, &amp;buf_type);

    <span class="kw">while</span>(<span class="dv">1</span>)
    {
        <span class="co">// Synchronize</span>
        <span class="kw">struct</span> pollfd fds = 
        {
            .fd = fd,
            .events = POLLIN
        };
        poll(&amp;fds, <span class="dv">1</span>, -<span class="dv">1</span>);

        <span class="co">// Dump buffer to stdout</span>
        <span class="kw">struct</span> v4l2_buffer buffer = 
        {
            .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
            .memory = V4L2_MEMORY_MMAP,
        };
        ioctl(fd, VIDIOC_DQBUF, &amp;buffer);
        write(<span class="dv">1</span>, pbuffers[buffer.index], buffer.bytesused);
        ioctl(fd, VIDIOC_QBUF, &amp;buffer);
    }
}</code></pre></td></tr></table>
<!-- -->

<h1 id="colorspace-conversion-example"><a href="#colorspace-conversion-example">Colorspace conversion example</a></h1>
<p>In this example RGB to YUV color-space conversion is implemented in fragment shader. Each input channel has its own texturing unit, texture coordinates are divided by sub-sampling factor 4:2:0.</p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="sourceCode"><pre><code class="sourceCode c">uniform sampler2D texY, texU, texV;
varying vec2 texCoord;

<span class="dt">void</span> main()
{
    <span class="dt">float</span> y = texture2D(texY, texCoord).a * <span class="fl">1.1644</span> - <span class="fl">0.062745</span>;
    <span class="dt">float</span> u = texture2D(texU, texCoord / <span class="dv">2</span>).a - <span class="fl">0.5</span>;
    <span class="dt">float</span> v = texture2D(texV, texCoord / <span class="dv">2</span>).a - <span class="fl">0.5</span>;

    gl_FragColor = vec4(
        y + <span class="fl">1.596</span> * v,
        y - <span class="fl">0.39176</span> * v - <span class="fl">0.81297</span> * u,
        y + <span class="fl">2.0172</span> * u,
        <span class="fl">1.0</span>);
}</code></pre></td></tr></table>
<h1 id="references" class="unnumbered"><a href="#references">References</a></h1>
<!-- -->

<div class="references">
<p>[1] BIMBER, Oliver and RASKAR, Ramesh. <em>Spatial augmented reality: merging real and virtual worlds</em>. Wellesley: A K Peters, 2005. ISBN 15-688-1230-2. </p>
<p>[2] Bit Wise. FreeRTOS. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://www.freertos.org/" title="http://www.freertos.org/">http://www.freertos.org/</a></p>
<p>[3] Wind River. VxWorks. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://www.windriver.com/products/vxworks" title="http://www.windriver.com/products/vxworks">http://www.windriver.com/products/vxworks</a></p>
<p>[4] Microsoft. Windows Embedded. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://www.microsoft.com/windowsembedded/en-us/windows-embedded.aspx" title="http://www.microsoft.com/windowsembedded/en-us/windows-embedded.aspx">http://www.microsoft.com/windowsembedded/en-us/windows-embedded.aspx</a></p>
<p>[5] LinuxTV Developers. Linux Media Infrastructure API. [online]. 2012. [cited 20 November 2013]. Available from: <a href="http://linuxtv.org/downloads/v4l-dvb-apis" title="http://linuxtv.org/downloads/v4l-dvb-apis">http://linuxtv.org/downloads/v4l-dvb-apis</a></p>
<p>[6] Consultative Committee on International Radio. Recommendation BT.601. [online]. 2011. [cited 20 November 2013]. Available from: <a href="http://www.itu.int/rec/R-REC-BT.601/en" title="http://www.itu.int/rec/R-REC-BT.601/en">http://www.itu.int/rec/R-REC-BT.601/en</a></p>
<p>[7] TI OMAP Developers. TI OMAP Trunk PPA. [online]. 2013. [cited 20 November 2013]. Available from: <a href="https://launchpad.net/~tiomap-dev/+archive/omap-trunk" title="https://launchpad.net/~tiomap-dev/+archive/omap-trunk">https://launchpad.net/~tiomap-dev/+archive/omap-trunk</a></p>
<p>[8] Khronos Group. OpenGL ES 2.x - for Programmable Hardware. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://www.khronos.org/opengles/2_X" title="http://www.khronos.org/opengles/2_X">http://www.khronos.org/opengles/2_X</a></p>
<p>[9] MUNSHI, Aaftab, GINSBURG, Dan and SHREINER, Dave. <em>OpenGLES 2.0 programming guide</em>. New York: Addison-Wesley Professional, 2009. ISBN 978-0-321-50279-7. </p>
<p>[10] JAZAR, Reza N. <em>Theory of applied robotics: kinematics, dynamics, and control</em>. 2nd ed. New York: Springer, 2010. ISBN 978-1-4419-1749-2. </p>
<p>[11] National Marine Electronics Association. NMEA 0183. [online]. 2008. [cited 20 November 2013]. Available from: <a href="http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp" title="http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp">http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp</a></p>
<p>[12] Garmin International. Garmin Proprietary NMEA 0183 Sentences. [online]. Available from: <a href="https://www8.garmin.com/support/pdf/NMEA_0183.pdf" title="https://www8.garmin.com/support/pdf/NMEA_0183.pdf">https://www8.garmin.com/support/pdf/NMEA_0183.pdf</a></p>
<p>[13] NIMA. Department of Defense World Geodetic System 1984. [online]. 1997. [cited 20 November 2013]. Available from: <a href="http://earth-info.nga.mil/GandG/publications/tr8350.2/wgs84fin.pdf" title="http://earth-info.nga.mil/GandG/publications/tr8350.2/wgs84fin.pdf">http://earth-info.nga.mil/GandG/publications/tr8350.2/wgs84fin.pdf</a></p>
<p>[14] WEISSTEIN, Eric W. <em>Spherical trigonometry</em> [online]. MathWorld. [cited 20 November 2013]. Available from: <a href="http://mathworld.wolfram.com/SphericalTrigonometry.html" title="http://mathworld.wolfram.com/SphericalTrigonometry.html">http://mathworld.wolfram.com/SphericalTrigonometry.html</a></p>
<p>[15] A, Furuti Carlos. Map Projections: Directions. [online]. 2014. Available from: <a href="http://www.progonos.com/furuti/MapProj/Normal/CartProp/Rhumb/rhumb.html" title="http://www.progonos.com/furuti/MapProj/Normal/CartProp/Rhumb/rhumb.html">http://www.progonos.com/furuti/MapProj/Normal/CartProp/Rhumb/rhumb.html</a></p>
<p>[16] MAHAMMAD, Sazid Sk. and R., Ramakrishnan. GeoTIFF - a Standard Image File Format for GIS Applications. [online]. [cited 14 April 2014]. Available from: <a href="http://www.gisdevelopment.net/technology/ip/mi03117pf.htm" title="http://www.gisdevelopment.net/technology/ip/mi03117pf.htm">http://www.gisdevelopment.net/technology/ip/mi03117pf.htm</a></p>
<p>[17] Texas Instruments. OMAP 4460 Multimedia Device. [online]. 2012. [cited 20 November 2013]. Available from: <a href="http://www.ti.com/product/omap4460" title="http://www.ti.com/product/omap4460">http://www.ti.com/product/omap4460</a></p>
<p>[18] mipi alliance. Camera Interface Specifications. [online]. 2014. Available from: <a href="http://mipi.org/specifications/camera-interface" title="http://mipi.org/specifications/camera-interface">http://mipi.org/specifications/camera-interface</a></p>
<p>[19] InvenSense. MPU-9150 Nine-axis MEMS MotionTracking™ Device. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://www.invensense.com/mems/gyro/mpu9150.html" title="http://www.invensense.com/mems/gyro/mpu9150.html">http://www.invensense.com/mems/gyro/mpu9150.html</a></p>
<p>[20] pandaboard.org. Pandaboard. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://pandaboard.org/content/resources/references" title="http://pandaboard.org/content/resources/references">http://pandaboard.org/content/resources/references</a></p>
<p>[21] beagleboard.org. BeagleBone Black. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://beagleboard.org/products/beaglebone%20black" title="http://beagleboard.org/products/beaglebone%20black">http://beagleboard.org/products/beaglebone%20black</a></p>
<p>[22] Texas Instruments. Sensor Hub BoosterPack. [online]. 2013. [cited 20 November 2013]. Available from: <a href="http://www.ti.com/tool/boostxl-senshub" title="http://www.ti.com/tool/boostxl-senshub">http://www.ti.com/tool/boostxl-senshub</a></p>
<p>[23] JAROS, Martin. <em>Augmented reality navigation</em> [online]. [cited 13 December 2013]. Available from: <a href="https://github.com/martinjaros/augmented-reality-navigation" title="https://github.com/martinjaros/augmented-reality-navigation">https://github.com/martinjaros/augmented-reality-navigation</a></p>
</div>
</body>
</html>
