<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Martin Jaroš" />
  <title>Augmented reality navigation</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title">Augmented reality navigation</h1>
<h2 class="author">Martin Jaroš</h2>
<h3 class="date">Dec. 2013</h3>
</div>
<div id="TOC">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#augmented-reality">Augmented reality</a><ul>
<li><a href="#design-goals">Design goals</a></li>
<li><a href="#hardware-limitations">Hardware limitations</a></li>
</ul></li>
<li><a href="#application">Application</a><ul>
<li><a href="#linux-kernel">Linux kernel</a></li>
<li><a href="#video-subsystem">Video subsystem</a></li>
<li><a href="#graphics-subsystem">Graphics subsystem</a></li>
<li><a href="#inertial-measurement-subsystem">Inertial measurement subsystem</a><ul>
<li><a href="#industrial-io-module">Industrial I/O module</a></li>
<li><a href="#dcm-algorithm">DCM algorithm</a></li>
</ul></li>
<li><a href="#satellite-navigation-subsystem">Satellite navigation subsystem</a></li>
</ul></li>
<li><a href="#hardware">Hardware</a></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendixa">Threads example</a></li>
<li><a href="#appendixb">Video capture example</a></li>
<li><a href="#appendixc">Colorspace conversion example</a></li>
</ul>
</div>
<h1 id="preface" class="unnumbered"><a href="#preface">Preface</a></h1>
<blockquote>
<p>Introduction <span class="citation">[1]</span></p>
</blockquote>
<h1 id="augmented-reality"><a href="#augmented-reality">Augmented reality</a></h1>
<h2 id="design-goals"><a href="#design-goals">Design goals</a></h2>
<blockquote>
<p>Project overview</p>
</blockquote>
<div class="figure">
<img src="images/arnav.svg" alt="Project overview" /><p class="caption">Project overview</p>
</div>
<h2 id="hardware-limitations"><a href="#hardware-limitations">Hardware limitations</a></h2>
<p>Application is designed to run in embedded environments, where power management is very important. While many platforms features multiple symmetrical processor cores - CPUs, application should focus on lowest per-core usage as possible. This can be done by delegating specific tasks to specialized hardware. CPU is specialized in execution of a single thread, integer and bitwise operations, with many branches in its code. With vector and floating point extensions they are also very efficient in computation of difficult mathematical algorithms. However they do not perform well in simple calculations over large amounts of data, where mass parallelization is possible. This is the case in graphics where special graphics processors - GPUs have been deployed. GPU consists of high number (hundreds) of simple cores, which are able to perform operations over large blocks of data. They scale efficiently with the amount of data needed to be processed due to parallelization, however they have problems with nonlinear code with branches. While CPUs have long pipelines for branch optimizations, GPUs cannot employ those, any branch in their code will be extremely inefficient and should be avoided. <a href="#graphics-subsystem">Graphics</a> chapter focuses on this area. There are also available specialized subsystems designed and optimized for a single purpose. For example video accelerators, capable of video encoding and decoding, image capture systems or peripheral drivers. They will be mentioned in specific chapters.</p>
<p>Developing an application for an embedded device faces a problem, as there are big differences between these devices it is hard to support the hardware and make the application portable. In order to reuse code and reduce application size, libraries are generally used to create an intermediary layer between application and the hardware. However, to provide enough abstraction some sort of operating system has to be used. Operating systems may be real-time, giving applications full control, behaving just like large libraries. This is favorable approach in embedded systems as it allows precise timings. There are many such systems specially tailored for embedded applications like <a href="http://www.freertos.org">FreeRTOS</a> or proprietary <a href="http://www.windriver.com/products/vxworks">VxWorks</a>. On the other hand, as recent processors improved greatly in power, efficiency and capabilities, it is possible and quite feasible to run a full featured system like <a href="http://www.elinux.org">Linux</a> or proprietary <a href="http://www.microsoft.com/windowsembedded">Windows CE</a>. Linux kernel is highly portable and configurable, although it does restrict applications from real-time use (Linux RT patches also exist for real-time applications), as all hardware dependent modules which requires full control over the hardware are part of the kernel itself, application does not need to run in real-time at all. Other advantages are free, open and well documented sources, highly standardized and POSIX compliant application interface, large amount of drivers with good manufacturer support. While its disadvantages are very large code base and steep learning curve, which may slow the initial development. Nevertheless Linux kernel has been chosen for the project, more details about its interfaces are in the <a href="#linux-kernel">Linux kernel</a> chapter. While the application is designed to be highly portable depending only on the kernel itself, several devices has been chosen as the reference, they are listed in the <a href="#hardware">hardware</a> chapter.</p>
<h1 id="application"><a href="#application">Application</a></h1>
<h2 id="linux-kernel"><a href="#linux-kernel">Linux kernel</a></h2>
<p>Programs running in Linux are divided into two groups, kernel-space and user-space. Only kernel and its runtime modules are allowed to execute in kernel-space, they have physical memory access and use CPU in real-time. All other programs runs as processes in user-space, they have virtual memory access, which means their memory addresses are translated to the physical addresses in the background. In Linux each process runs in a sandbox, isolated from the rest of the system. Processes access memory unique to them, they cannot access memory assigned for other processes nor memory managed by the kernel. They may communicate with the outside environment by several means:</p>
<ul>
<li>Arguments and environment variables</li>
<li>Standard input, output and error output</li>
<li>Virtual File System</li>
<li>Signals</li>
<li>Sockets</li>
<li>Memory mapping</li>
</ul>
<p>Each process is ran with several arguments in a specific environment with three default file descriptors. For example running</p>
<p><code class="sourceCode bash"><span class="ot">VARIABLE=</span>value <span class="kw">./executable</span> argument1 argument2 <span class="kw">&lt;</span>input <span class="kw">1&gt;</span>output <span class="kw">2&gt;</span>error</code></p>
<p>will execute <code>executable</code> with environment variable <code>VARIABLE</code> of value <code>value</code> with two arguments <code>argument1</code> and <code>argument2</code>. Standard input will be read from file <code>input</code> while regular output will be written to file <code>output</code> and error output to file <code>error</code>. This process may further communicate by accessing files in the Virtual File System, kernel may expose useful process information for example via <code>procfs</code> file-system usually mounted at <code>/proc</code>. Other types of communication are signals (which may be sent between processes or by kernel) and network sockets. With internal network loop-back device, network style inter process communication is possible using standard protocols (UDP, TCP, ...). Memory mapping is a way to request access to some part of the physical memory.</p>
<p>Process execution is not real-time, but they are assigned restricted processor time by the kernel. They may run in numerous threads, each thread has preemptively scheduled execution. Threads share memory within a process, memory access to these shared resources must done with care to avoid race conditions and data corruption. Kernel provides <em>mutex</em> objects to lock threads and avoid simultaneous memory access. Each shared resource should be attached to a <em>mutex</em>, which is locked during access to this resource. Thread must not lock <em>mutex</em> while still holding lock to this or any other <em>mutex</em> in order to avoid dead-locking. Source example on how to use threads is in the <a href="#appendixa">appendix A</a>.</p>
<p>Linux kernel has monolithic structure, so all device drivers resides in the kernel-space. From application point of view, this means that all peripheral access must be done through the standard library and Virtual File System. Individual devices are accessible as device files defined by major and minor number typically located at <code>/dev</code>. These files could be created automatically by kernel (<code>devtmpfs</code> file-system), by daemon (<a href="http://linux.die.net/man/8/udev"><code>udev(8)</code></a>)), or manually by <a href="http://linux.die.net/man/1/mknod"><code>mknod(1)</code></a>. Complete kernel device model is exported as <code>sysfs</code> file-system and typically mounted at <code>/sys</code>.</p>
<table>
<caption>Available functions for working with device file descriptors</caption>
<thead>
<tr class="header">
<th align="left"><strong>Function name</strong></th>
<th align="left"><strong>Access type</strong></th>
<th align="left"><strong>Typical usage</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="http://linux.die.net/man/2/select"><code>select()</code></a>, <a href="http://linux.die.net/man/2/poll"><code>poll()</code></a></td>
<td align="left">event</td>
<td align="left">Synchronization, multiplexing, event handling</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linux.die.net/man/2/ioctl"><code>ioctl()</code></a></td>
<td align="left">structure</td>
<td align="left">Configuration, register access</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linux.die.net/man/2/read"><code>read()</code></a>, <a href="http://linux.die.net/man/2/write"><code>write()</code></a></td>
<td align="left">stream</td>
<td align="left">Raw data buffers, byte streams</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linux.die.net/man/2/mmap"><code>mmap()</code></a></td>
<td align="left">block</td>
<td align="left">High throughput data transfers</td>
</tr>
</tbody>
</table>
<p>For example let's assume a generic peripheral device connected by the I<sup>2</sup>C bus. First, to tell kernel there is such a device, the <code>sysfs</code> file-system may be used</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> <span class="ot">$DEVICE_NAME</span> <span class="ot">$DEVICE_ADDRESS</span> <span class="kw">&gt;</span> /sys/bus/i2c/devices/i2c-1/new_device</code></p>
<p>This should create a special file in <code>/dev</code>, which should be opened by <a href="http://linux.die.net/man/2/open"><code>open()</code></a> to get a file descriptor for this device. Device driver may export some <em>ioctl</em> requests, each request is defined by a number and a structure passed between the application and the kernel. Driver should define requests for controlling the device, maybe accessing its internal registers and configuring a data stream. Each request is called by</p>
<p><code class="sourceCode c">ioctl(fd, REQNUM, &amp;data);</code></p>
<p>where <code>fd</code> is the file descriptor, <code>REQNUM</code> is the request number defined in the driver header and <code>data</code> is the structure passed to the kernel. This request will be synchronously processed by the kernel and the result stored in the <code>data</code> structure. Let's assume this devices has been configured to stream an integer value every second to the application. To synchronize with this timing application may use</p>
<p><code class="sourceCode c"><span class="kw">struct</span> pollfd fds = {fd, POLLIN};</code><br /><code class="sourceCode c">poll(&amp;fds, <span class="dv">1</span>, -<span class="dv">1</span>);</code></p>
<p>which will block infinitely until there is a value ready to be read. To actually read it,</p>
<p><code class="sourceCode c"><span class="dt">int</span> buffer[<span class="dv">1</span>];</code><br /><code class="sourceCode c">ssize_t num = read(fd, buffer, <span class="kw">sizeof</span>(buffer));</code></p>
<p>will copy this value to the buffer. Copying causes performance issues if there are very large amounts of data. To access this data directly without copying them, application has to map physical memory used by the driver. This allows for example direct access to a DMA channel, it should be noted that this memory may still be needed by kernel, so there should be some kind of dynamic access restriction, possibly via <em>ioctl</em> requests (this would be driver specific).</p>
<h2 id="video-subsystem"><a href="#video-subsystem">Video subsystem</a></h2>
<p>Video support in Linux kernel is maintained by the <a href="http://linuxtv.org">LinuxTV</a> project, it implements the <code>videodev2</code> kernel module and defines the <em>V4L2</em> interface. Modules are part of the mainline kernel at <code>drivers/media/video/*</code> with header <code>linux/videodev2.h</code>. The core module is enabled by the <code>VIDEO_V4L2</code> configuration option, specific device drivers should be enabled by their respective options. V4L2 is the latest revision and is the most widespread video interface throughout Linux, drives are available from most hardware manufactures and usually mainlined or available as patches. The <a href="http://linuxtv.org/downloads/v4l-dvb-apis">Linux Media Infrastructure API</a><span class="citation">[2]</span> is a well documented interface shared by all devices. It provides abstraction layer for various device implementations, separating the platform details from the applications. Each video device has its device file and is controlled via <em>ioctl</em> calls. For streaming standard I/O functions are supported, but the memory mapping is preferred, this allows passing only pointers between the application and the kernel, instead of unnecessary copying the data around.</p>
<table>
<caption>V4L2 ioctl calls defined in <code>linux/videodev2.h</code></caption>
<thead>
<tr class="header">
<th align="left"><strong>Name</strong></th>
<th align="left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-querycap.html">VIDIOC_QUERYCAP</a></td>
<td align="left">Query device capabilities</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-g-fmt.html">VIDIOC_G_FMT</a></td>
<td align="left">Get the data format</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-g-fmt.html">VIDIOC_S_FMT</a></td>
<td align="left">Set the data format</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-reqbufs.html">VIDIOC_REQBUFS</a></td>
<td align="left">Initiate memory mapping</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-querybuf.html">VIDIOC_QUERYBUF</a></td>
<td align="left">Query the status of a buffer</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-qbuf.html">VIDIOC_QBUF</a></td>
<td align="left">Enqueue buffer to the kernel</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-qbuf.html">VIDEOC_DQBUF</a></td>
<td align="left">Dequeue buffer from the kernel</td>
</tr>
<tr class="even">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-streamon.html">VIDIOC_STREAMON</a></td>
<td align="left">Start streaming</td>
</tr>
<tr class="odd">
<td align="left"><a href="http://linuxtv.org/downloads/v4l-dvb-apis/vidioc-streamon.html">VIDIOC_STREAMOFF</a></td>
<td align="left">Stop streaming</td>
</tr>
</tbody>
</table>
<p>Application sets the format first, then requests and maps buffers from the kernel. Buffers are exchanged between the kernel and the application. When the buffer is enqueued, it will be available for the kernel to capture data to it. When the buffer is dequeued, kernel will not access the buffer and application may read the data. After all buffer are enqueued application starts the stream. Polling is used to wait for the kernel until it fills the buffer, buffer should not be accessed simultaneously by the kernel and the application. After processing the buffer, application should return it back to the kernel queue. Note that buffers should be properly unmapped by the application after stopping the stream.</p>
<div class="figure">
<img src="images/v4l2capture.svg" alt="V4L2 capture" /><p class="caption">V4L2 capture</p>
</div>
<p>Source example for simple video capture is in <a href="#appendixb">appendix B</a>. The image format is specified using the little-endian four-character code (FOURCC). V4L2 defines several formats and provides <code>v4l2_fourcc()</code> macro to create a format code from four characters. As described later in the <a href="#graphics-subsystem">graphics subsystem</a> chapter, graphics uses natively the RGB4 format. This format is defined as a single plane with one sample per pixel and four bytes per sample. These bytes represents red, green and blue channel values respectively. Image size is therefore <span class="math">\(width \cdot height \cdot 4\)</span> bytes. Many image sensors however support YUV color-space, for example the YU12 format. This one is defined as three planes, the first plane with one luminance sample per pixel and the second and third plane with one chroma sample per four pixels (2 pixels per row, interleaved). Each sample has one byte, this format is also referenced as YUV 4:2:0 and its image size is <span class="math">\(width \cdot height \cdot 1.5\)</span> bytes. The luminance and chroma of a pixel is defined as</p>
<p><span class="math">\(E_Y = W_R \cdot E_R + (1-W_R-W_B) \cdot E_G + W_B \cdot E_B\)</span>,</p>
<p><span class="math">\(E_{C_r} = \dfrac {0.5 (E_R - E_Y)} {1 - W_R}\)</span>,</p>
<p><span class="math">\(E_{C_b} = \dfrac {0.5 (E_B - E_Y)} {1 - W_B}\)</span>,</p>
<p>where <em>E<sub>R</sub></em>, <em>E<sub>G</sub></em>, <em>E<sub>B</sub></em> are normalized color values and <em>W<sub>R</sub></em>, <em>W<sub>B</sub></em> are their weights. <a href="http://www.itu.int/rec/R-REC-BT.601/en">ITU-R Rec. BT.601</a><span class="citation">[3]</span> defines weights as 0.299 and 0.114 respectively, it also defines how they are quantized</p>
<p><span class="math">\(Y = 219 E_Y + 16\)</span>,</p>
<p><span class="math">\(C_r = 224 E_{C_r} + 128\)</span>,</p>
<p><span class="math">\(C_b = 224 E_{C_b} + 128\)</span>.</p>
<p>To calculate <em>R</em>, <em>G</em>, <em>B</em> values from <em>Y</em>, <em>Cr</em>, <em>Cb</em> values, inverse formulas must be used</p>
<p><span class="math">\(E_Y = \dfrac {Y - 16} {219}\)</span>,</p>
<p><span class="math">\(E_{C_r} =  \dfrac {C_r - 128} {224}\)</span>,</p>
<p><span class="math">\(E_{C_b} = \dfrac {C_b - 128} {224}\)</span>,</p>
<p><span class="math">\(E_R = E_Y + 2 E_{C_r} (1 - W_R)\)</span>,</p>
<p><span class="math">\(E_G = E_Y - 2 E_{C_r} \dfrac {W_R - {W_R}^2} {W_G} - 2 E_{C_b} \dfrac {W_B - {W_B}^2} {W_G}\)</span>,</p>
<p><span class="math">\(E_B = E_Y + 2 E_{C_b} (1 - W_B)\)</span>.</p>
<p>It should be noted that not all devices may use the BT.601 recommendation, V4L2 refers to it as <code>V4L2_COLORSPACE_SMPTE170M</code> in the <code>VIDIOC_S_FMT</code> request structure. Implementation of the YUV to RGB color-space conversion is most efficient on graphics accelerators, such example is included in <a href="#appendixc">appendix C</a>. It is written in GLSL for fragment processor, see <a href="#graphics-subsystem">graphics subsystem</a> chapter for further description.</p>
<p>There is a kernel module <code>v4l2loopback</code> which creates a video loop-back device, similar to network loop-back, allowing piping two video applications together. This is very useful not only for testing, but also for implementation of intermediate decoders. <a href="http://gstreamer.freedesktop.org">GStreamer</a> is a powerful multimedia framework widespread in Linux distributions, composed of a core infrastructure and hundreds of plug-ins. This command will create synthetic RGB4 video stream for the application, useful for testing</p>
<p><code class="sourceCode bash"><span class="kw">modprobe</span> v4l2loopback</code><br /><code class="sourceCode bash"><span class="kw">gst-launch</span> videotestsrc pattern=solid-color foreground-color=0xE0F0E0 ! \</code><br /><code class="sourceCode bash"><span class="st">&quot;video/x-raw,format=RGBx,width=800,height=600,framerate=20/1&quot;</span> <span class="kw">\</span></code><br /><code>! v4l2sink device=/dev/video0</code></p>
<p>Texas Instruments distributes a <a href="https://launchpad.net/~tiomap-dev/+archive/omap-trunk">meta package</a><span class="citation">[4]</span> for their OMAP platform featuring all required modules and DSP firmware. This includes kernel modules for <em>SysLink</em> inter-chip communication library, <em>Distributed Codec Engine</em> library and <em>ducati</em> plug-in for GStreamer. With the meta-package installed, it is very easy and efficient to implement mainstream encoded video formats. For example following command will create GStreamer pipeline to receive video payload over a network socket from an IP camera, decode it and push it to the loop-back device for the application. MPEG-4 AVC (H.264) decoder of the IVA 3 is used in this example.</p>
<p><code class="sourceCode bash"><span class="kw">modprobe</span> v4l2loopback</code><br /><code class="sourceCode bash"><span class="kw">gst-launch</span> udpsrc port=5004 caps=\</code><br /><code class="sourceCode bash"><span class="st">&quot;application/x-rtp,media=video,payload=96,clock-rate=90000,encoding-name=H264&quot;</span> <span class="kw">\</span></code><br /><code>! rtph264depay ! h264parse ! ducatih264dec ! v4l2sink device=/dev/video0</code></p>
<p>On OMAP4460 this would consume only about 15% of the CPU time as the decoding is done by the IVA 3 video accelerator in parallel to the CPU which only passes pointers around and handles synchronization. Output format is NV12 which is similar to YU12 format described earlier, but there is only one chroma plane with two-byte samples, first byte being the U channel and the second byte the V channel, sampling is same 4:2:0. The YUV to RGB color space conversion must take place here, preferably implemented on the GPU as described above.</p>
<p>Cortex-A9 cores on the OMAP4460 also have the NEON co-processor, capable of vector floating point math. Although not very supported by the GCC C compiler, there are many assembly written libraries implementing coders with the NEON acceleration. For example the <a href="http://www.libjpeg-turbo.org/"><em>libjpeg-turbo</em></a> library is implementing the <em>libjpeg</em> interface. It is useful for USB cameras, as the USB throughput is not high enough for raw high definition video, but is sufficient with JPEG coding (as most USB cameras supports JPEG, but does not support H.264). 1080p JPEG stream decoded with this library via its GStreamer plug-in will consume about 90% of the single CPU core time (note that there are two CPU cores available). However, comparable to the AVC, JPEG encoding will cause visible quality degradation in the raw stream (video looks grainy).</p>
<h2 id="graphics-subsystem"><a href="#graphics-subsystem">Graphics subsystem</a></h2>
<blockquote>
<p>Graphics stack, OpenGL ES 2.0 <span class="citation">[5]</span></p>
</blockquote>
<h2 id="inertial-measurement-subsystem"><a href="#inertial-measurement-subsystem">Inertial measurement subsystem</a></h2>
<p>Application needs to know its spatial orientation for rendering, there three devices which may provide such information, gyroscope, compass (magnetometer) and accelerometer. Hardware details about these devices are in the <a href="#hardware">hardware</a> chapter.</p>
<h3 id="industrial-io-module"><a href="#industrial-io-module">Industrial I/O module</a></h3>
<p>A relatively young kernel module <code>iio</code> has been implemented in recent kernels to provide standardized support for sensors and analog converters typically connected by I<sup>2</sup>C bus. While many device drivers are still in staging tree, to core module is ready for production code. Subsystem provides device structure mapped in <code>sysfs</code>, typically available at <code>/sys/bus/iio/devices/</code>. Device are implemented usually on top of the <code>i2c-dev</code> driver and registered as <code>/sys/bus/iio/devices/iio:deviceX</code>, where X is the device number and the device name may be obtained by</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:deviceX/name</code></p>
<p>There are many possible channels, named by the value type they represents. To read an immediate value, for example from an ADC channel 1</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:deviceX/in_voltage1_raw</code><br /><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:deviceX/in_voltage_scale</code></p>
<p>where the result value in volts is <span class="math">\(raw \cdot scale\)</span>. However, being easy, this is not efficient, buffers have been implemented to stream measured data to the application. Buffer uses device file named after the <code>iio</code> device, e.g. <code>/dev/iio:deviceX</code>. To stream data through the buffer, driver needs to have control over the timing, triggers have been implemented for this purpose. They are accessible as <code>/sys/bus/iio/devices/triggerX</code>, where X is the trigger number and its name may be obtained by</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/triggerX/name</code></p>
<p>Software trigger may be created by</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/iio_sysfs_trigger/add_trigger</code></p>
<p>and triggered by application</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/trigger0/trigger_now</code></p>
<p>Name of this trigger is <code>sysfstrigX</code>, where X is the trigger number. Hardware triggers are also implemented, both GPIO and timer based triggers. Devices may implement triggers themselves, providing for example the data ready trigger. Device triggers are generally named as <code>name-devX</code>, where <code>name</code> is device name and <code>X</code> is device number. To use trigger with the buffer use</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> <span class="st">&quot;triggername&quot;</span> <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:deviceX/trigger/current_trigger</code></p>
<p>where <code>triggername</code> is the name of the trigger, for example <code>adc-dev0</code> will be the device trigger for the ADC. Data are measured in specific channels, they are defined in <code>/sys/bus/iio/devices/iio:device0/scan_elements</code>. Channels must be enabled for buffering individually, for example</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltage1_en</code><br /><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltage2_en</code></p>
<p>will enable ADC channels 1 and 2. Buffer itself can be started by</p>
<p><code class="sourceCode bash"><span class="kw">echo</span> 256 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:deviceX/buffer/length</code><br /><code class="sourceCode bash"><span class="kw">echo</span> 1 <span class="kw">&gt;</span> /sys/bus/iio/devices/iio:deviceX/buffer/enabled</code></p>
<p>this will start streaming data to the device file. Data are formatted in packets, each packed consists of per-channel values and is terminated by 8 byte time-stamp of the sample. Order of the channels in the buffer can be obtained by</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltageX_index</code></p>
<p>which reads index of the specified channel. Data format of this channel is</p>
<p><code class="sourceCode bash"><span class="kw">cat</span> /sys/bus/iio/devices/iio:device0/scan_elements/in_voltageX_type</code></p>
<p>which reads encoded string, for example <code>le:u10/16&gt;&gt;0</code>, where <code>le</code> means little-endian, <code>u</code> means unsigned, <code>10</code> is the number of relevant bits while <code>16</code> is the number of actual bits and <code>0</code> is the number of right shifts needed.</p>
<p>Following channels are needed by the application:</p>
<ul>
<li><code>anglvel_x</code></li>
<li><code>anglvel_y</code></li>
<li><code>anglvel_z</code></li>
<li><code>accel_x</code></li>
<li><code>accel_y</code></li>
<li><code>accel_z</code></li>
<li><code>magn_x</code></li>
<li><code>magn_y</code></li>
<li><code>magn_z</code></li>
</ul>
<p>representing measurements from gyroscope, accelerometer and magnetometer respectively.</p>
<h3 id="dcm-algorithm"><a href="#dcm-algorithm">DCM algorithm</a></h3>
<p>Equations needed to calculate device attitude have been derived from <span class="citation">[6]</span>. Gyroscope measures angular speed around device axes, it offers high differential precision and fast sampling rate, however it suffers slight zero offset error. Device attitude can be obtained simply by integrating measured angular rates, provided that initial attitude is known. The angular rate is defined as</p>
<p><span class="math">\(\overrightarrow{\omega_g} = \frac{\mathrm{d}}{\mathrm{d}t} \overrightarrow{\Phi}_{(t)}\)</span>,</p>
<p>so the angular displacement between last two samples is</p>
<p><span class="math">\(\left [\Phi_x, \Phi_y, \Phi_z \right ] = \left [ \omega_x, \omega_y, \omega_z \right ] \cdot _\Delta t\)</span>.</p>
<p>This can be described as a rotation</p>
<p><span class="math">\(\mathbf{R}_{gyro} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\  0 &amp; \cos(\Phi_x) &amp; -\sin(\Phi_x) \\  0 &amp; \sin(\Phi_x) &amp; \cos(\Phi_x) \end{bmatrix} \times \begin{bmatrix} \cos(\Phi_y) &amp; 0 &amp; \sin(\Phi_y) \\  0 &amp; 1 &amp; 0 \\  -\sin(\Phi_y) &amp; 0 &amp; \cos(\Phi_y) \end{bmatrix} \times \begin{bmatrix} \cos(\Phi_z) &amp; -\sin(\Phi_y) &amp; 0 \\  \sin(\Phi_y) &amp; \cos(\Phi_y) &amp; 0 \\  0 &amp; 0 &amp; 1 \end{bmatrix}\)</span>.</p>
<p>With <span class="math">\(_\Delta t\)</span> close to zero a small-angle approximation may be used to simplify <span class="math">\(\cos(x)=1\)</span>, <span class="math">\(\sin(x)=x\)</span></p>
<p><span class="math">\(\mathbf{R}_{gyro} \doteq \begin{bmatrix} 1 &amp; - \Phi_z &amp; \Phi_y \\ \Phi_x \Phi_y + \Phi_z &amp; 1 - \Phi_x \Phi_y \Phi_z &amp; - \Phi_x \\ \Phi_x \Phi_z - \Phi_y &amp; \Phi_x + \Phi_y \Phi_z &amp; 1 \end{bmatrix}\)</span>.</p>
<p>Let's define the directional cosine matrix describing device attitude</p>
<p><span class="math">\(\mathbf{DCM} = \begin{bmatrix} \widehat{\mathbf{I}}\cdot \widehat{\mathbf{x}} &amp; \widehat{\mathbf{I}}\cdot \widehat{\mathbf{y}} &amp; \widehat{\mathbf{I}}\cdot \widehat{\mathbf{z}} \\ \widehat{\mathbf{J}}\cdot \widehat{\mathbf{x}} &amp; \widehat{\mathbf{J}}\cdot \widehat{\mathbf{y}} &amp; \widehat{\mathbf{J}}\cdot \widehat{\mathbf{z}} \\  \widehat{\mathbf{K}}\cdot \widehat{\mathbf{x}} &amp; \widehat{\mathbf{K}}\cdot \widehat{\mathbf{y}} &amp; \widehat{\mathbf{K}}\cdot \widehat{\mathbf{z}} \end{bmatrix} = \begin{bmatrix} \widehat{\mathbf{I}}_{xyz} \\ \widehat{\mathbf{J}}_{xyz} \\ \widehat{\mathbf{K}}_{xyz} \end{bmatrix}\)</span>,</p>
<p>where <span class="math">\(\widehat{\mathbf{I}}\)</span> points to the north, <span class="math">\(\widehat{\mathbf{J}}\)</span> points to the east, <span class="math">\(\widehat{\mathbf{K}}\)</span> points to the ground and therefore <span class="math">\(\widehat{\mathbf{I}} = \widehat{\mathbf{J}} \times \widehat{\mathbf{K}}\)</span>. Roll, pitch and yaw angels in this matrix are</p>
<p><span class="math">\(\gamma = - \arctan_2 \left ( \dfrac {\mathbf{DCM}_{23}}{\mathbf{DCM}_{33}} \right )\)</span>,</p>
<p><span class="math">\(\beta = \arcsin (\mathbf{DCM}_{13})\)</span>,</p>
<p><span class="math">\(\alpha = - \arctan_2 \left ( \dfrac {\mathbf{DCM}_{12}}{\mathbf{DCM}_{11}} \right )\)</span>.</p>
<p>DCM can be computed by applying consecutive rotations over time</p>
<p><span class="math">\(\mathbf{DCM}_{(t)}  =  \mathbf{R}_{gyro(t)} \times \mathbf{DCM}_{(t-1)}\)</span>.</p>
<p>If the sampling rate is high enough (over 1kHz at least), this method is very accurate and has good dynamics over short periods of time, but in longer runs errors integrated during processing will cause serious drift (both numerical errors and zero offset errors). To mitigate these problems accelerometer and compass has to be used to provide the initial attitude and to fix the drift over time. Accelerometer measures external mechanical forces applied to the device together with gravitational force. However precision of these devices are generally worse and they have slower sampling rates. If there are no extern forces, it will measure the gravitational vector directly, thus providing the third row of the DCM</p>
<p><span class="math">\(\overrightarrow{\mathbf{a}}_{acc} = g~ \widehat{\mathbf{K}}_{xyz} + \dfrac{\overrightarrow{\mathbf{F}}}{m}\)</span>,</p>
<p><span class="math">\(\overrightarrow{\mathbf{F}} = 0 ~\rightarrow~ \widehat{\mathbf{K}}_{xyz} =  \dfrac {\overrightarrow{\mathbf{a}}_{acc}} {\left | \overrightarrow{\mathbf{a}}_{acc} \right |}\)</span>.</p>
<p>When there is an external force <span class="math">\(\overrightarrow{\mathbf{F}}\)</span> applied, which is not parallel and has significant magnitude relative to gravitational force <span class="math">\(m\,g\,\widehat{\mathbf{K}}_{xyz}\)</span>, measurements will degrade rapidly reaching singularity during the free fall (<span class="math">\(\left | \overrightarrow{\mathbf{a}}_{acc} \right | = 0\)</span>). This error may be corrected by using device speed measured by satellite navigation system with high sample rate (over 10Hz)</p>
<p><span class="math">\(\widehat{\mathbf{K}}_{xyz} = \dfrac {\overrightarrow{\mathbf{a}}_{acc} - \frac{\mathrm{d}}{\mathrm{d}t} {\overrightarrow{\mathbf{v}}_{GPS}}} {g}\)</span>.</p>
<p>Magnetometer has similar properties, it measures magnetic flux density of the field the device is within. This should ideally result in a vector pointing to the north, therefore providing the first row of the DCM</p>
<p><span class="math">\(\widehat{\mathbf{I}}_{xyz} = \dfrac {\overrightarrow{\mathbf{B}}_{corr}} {\left | \overrightarrow{\mathbf{B}}_{corr} \right |}\)</span>.</p>
<p>Magnetometers have even slower sampling rates and far worse precision as the Earth field is distorted by nearby metal objects. This magnetic deviation can be divided into hard-iron and soft-iron effects. Hard-iron distortion is caused by materials that produces magnetic field, that is added to the Earth magnetic field. Vector of this field can be subtracted to compensate this error</p>
<p><span class="math">\(\overrightarrow{\mathbf{B}}_{corr1} = \overrightarrow{\mathbf{B}}_{mag} - \frac{1}{2} \left [ \min(B_x) + \max(B_x), \min(B_y) + \max(B_y), \min(B_z) + \max(B_z) \right ]\)</span>.</p>
<p>The soft-iron distortion is caused by soft magnetic materials, which reshapes the field in a way that is not simply additive. It may be observed as an ellipse when the device is rotated around and the measured values are plotted. Compensating for these effects is involves remapping this ellipse back to the sphere. This is computation intensive and as soft-iron effects are usually weak (up to few degrees), it may be omitted.</p>
<p>Further more the magnetic field of the Earth itself does not point to the geographic north, but is rotated by an angle specific to the location on the Earth surface. Magnetic inclination is the vertical portion of this rotation causing magnetic vector to incline to the ground, it may be fixed by using measurements from the accelerometer to make the magnetic vector perpendicular to the gravitational vector</p>
<p><span class="math">\(\overrightarrow{\mathbf{B}}_{corr2} = \widehat{\mathbf{K}}_{xyz} \times \overrightarrow{\mathbf{B}}_{mag} \times \widehat{\mathbf{K}}_{xyz}\)</span>.</p>
<p>Magnetic declination (sometimes referred as magnetic variation) is the horizontal portion of this rotation and is sometimes provided by the satellite navigation systems. To correct for this error, measured values have to be rotated by the inverse angle</p>
<p><span class="math">\(\overrightarrow{\mathbf{B}}_{corr3} = \overrightarrow{\mathbf{B}}_{mag} \begin{bmatrix} \cos(var) &amp; \sin(var) \\ - \sin(var) &amp; \cos(var) \end{bmatrix}\)</span>.</p>
<p>By combination of the corrected results from accelerometer and magnetometer complete DCM can be calculated. Weighted average should be used, in real-time this yields</p>
<p><span class="math">\(\mathbf{DCM}_{(t)}  =  W_{gyro}~ (\mathbf{R}_{gyro} \times \mathbf{DCM}_{(t-1)}) + (1 - W_{gyro}) \begin{bmatrix} \widehat{\mathbf{I}}_{xyz} \\ \widehat{\mathbf{K}}_{xyz} \times \widehat{\mathbf{I}}_{xyz} \\ \widehat{\mathbf{K}}_{xyz} \end{bmatrix}\)</span>,</p>
<p>where <span class="math">\(\widehat{\mathbf{I}}_{xyz}\)</span> and <span class="math">\(\widehat{\mathbf{K}}_{xyz}\)</span> are calculated from magnetometer and accelerometer measurements. <em>W<sub>gyro</sub></em> is the weight of the gyroscope measurement, it must be estimated by trial and error to mitigate its drift but not add too much noise.</p>
<h2 id="satellite-navigation-subsystem"><a href="#satellite-navigation-subsystem">Satellite navigation subsystem</a></h2>
<blockquote>
<p>TTY module, stty, socat, GPS <span class="citation">[7]</span>, NMEA 0183 <span class="citation">[8]</span></p>
</blockquote>
<h1 id="hardware"><a href="#hardware">Hardware</a></h1>
<p><strong><a href="http://www.ti.com/product/omap4460">OMAP4460</a> application processor</strong><span class="citation">[9]</span></p>
<ul>
<li>two ARM Cortex-A9 SMP general-purpose processors</li>
<li>IVA 3 video accelerator, 1080p capable</li>
<li>image signal processor, 20MP capable</li>
<li>SGX540 3D graphics accelerator, OpenGL ES 2.0 compatible</li>
<li>HDMI v1.3 video output</li>
</ul>
<p><strong><a href="http://www.invensense.com/mems/gyro/mpu9150.html">MPU-9150</a> motion tracking device</strong><span class="citation">[10]</span></p>
<ul>
<li>embedded MPU-6050 3-axis gyroscope and accelerometer</li>
<li>embedded AK8975 3-axis digital compass</li>
<li>fully programmable, I<sup>2</sup>C interface</li>
</ul>
<p><strong><a href="http://www.ovt.com/products/sensor.php?id=93">OV5640</a> image sensor</strong></p>
<ul>
<li>1080p, 5MP resolution</li>
<li>raw RGB or YUV output</li>
</ul>
<h1 id="conclusion"><a href="#conclusion">Conclusion</a></h1>
<blockquote>
<p>Conclusion</p>
</blockquote>
<h1 id="appendixa"><a href="#appendixa">Threads example</a></h1>
<p>In this example two threads share standard input and output, access is restricted by <em>mutex</em> so only one thread may use the shared resource at any time.</p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="sourceCode"><pre><code class="sourceCode c"><span class="ot">#include &lt;stdio.h&gt;</span>
<span class="ot">#include &lt;pthread.h&gt;</span>

<span class="dt">void</span> *worker(<span class="dt">void</span> *arg)
{
    <span class="dt">static</span> <span class="dt">int</span> counter = <span class="dv">1</span>;
    pthread_mutex_t *mutex = (pthread_mutex_t*)arg;
    <span class="dt">char</span> *buffer;

    <span class="co">// Lock mutex to restrict access to stdin and stdout</span>
    pthread_mutex_lock(mutex);
    printf(<span class="st">&quot;This is worker %d, enter something: &quot;</span>, counter++);
    scanf(<span class="st">&quot;%ms&quot;</span>, &amp;buffer);
    pthread_mutex_unlock(mutex);

    <span class="kw">return</span> (<span class="dt">void</span>*)buffer;
}

<span class="dt">int</span> main()
{
    pthread_mutex_t mutex;
    pthread_t thread1, thread2;
    <span class="dt">char</span> *retval1, *retval2;

    <span class="co">// Initialize two threads with shared mutex, use default parameters</span>
    pthread_mutex_init(&amp;mutex, NULL);
    pthread_create(&amp;thread1, NULL, worker, (<span class="dt">void</span>*)&amp;mutex);
    pthread_create(&amp;thread2, NULL, worker, (<span class="dt">void</span>*)&amp;mutex);

    <span class="co">// Wait for both threads to finish and display results</span>
    pthread_join(thread1, (<span class="dt">void</span>**)&amp;retval1);
    pthread_join(thread2, (<span class="dt">void</span>**)&amp;retval2);
    printf(<span class="st">&quot;Thread 1 returned with `%s`.</span><span class="ch">\n</span><span class="st">&quot;</span>, retval1);
    printf(<span class="st">&quot;Thread 2 returned with `%s`.</span><span class="ch">\n</span><span class="st">&quot;</span>, retval2);

    pthread_mutex_destroy(&amp;mutex);
    <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre></td></tr></table>
<h1 id="appendixb"><a href="#appendixb">Video capture example</a></h1>
<p>In this example video device is configured to capture frames using memory mapping. These frames are dumped to standard output, instead of further processing.</p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
</pre></td><td class="sourceCode"><pre><code class="sourceCode c"><span class="ot">#include &lt;fcntl.h&gt;</span>
<span class="ot">#include &lt;unistd.h&gt;</span>
<span class="ot">#include &lt;poll.h&gt;</span>
<span class="ot">#include &lt;sys/mman.h&gt;</span>
<span class="ot">#include &lt;sys/ioctl.h&gt;</span>
<span class="ot">#include &lt;linux/videodev2.h&gt;</span>

<span class="dt">int</span> main()
{
    <span class="co">// Open device</span>
    <span class="dt">int</span> fd = open(<span class="st">&quot;/dev/video0&quot;</span>, O_RDWR | O_NONBLOCK);

    <span class="co">// Set video format</span>
    <span class="kw">struct</span> v4l2_format format =
    {
        .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
        .fmt =
        {
            .pix =
            {
                .width = <span class="dv">320</span>,
                .height = <span class="dv">240</span>,
                .pixelformat = V4L2_PIX_FMT_RGB32,
                .field = V4L2_FIELD_NONE,
                .colorspace = V4L2_COLORSPACE_SMPTE170M,
            },
        },
    };
    ioctl(fd, VIDIOC_S_FMT, &amp;format);

    <span class="co">// Request buffers</span>
    <span class="kw">struct</span> v4l2_requestbuffers requestbuffers =
    {
        .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
        .memory = V4L2_MEMORY_MMAP,
        .count = <span class="dv">4</span>,
    };
    ioctl(fd, VIDIOC_REQBUFS, &amp;requestbuffers);
    <span class="dt">void</span> *pbuffers[requestbuffers.count];

    <span class="co">// Map and enqueue buffers</span>
    <span class="dt">int</span> i;
    <span class="kw">for</span>(i = <span class="dv">0</span>; i &lt; requestbuffers.count; i++)
    {
        <span class="kw">struct</span> v4l2_buffer buffer = 
        {
            .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
            .memory = V4L2_MEMORY_MMAP,
            .index = i,
        };
        ioctl(fd, VIDIOC_QUERYBUF, &amp;buffer);
        pbuffers[i] = mmap(NULL, buffer.length,
                           PROT_READ | PROT_WRITE, MAP_SHARED,
                           fd, buffer.m.offset);
        ioctl(fd, VIDIOC_QBUF, &amp;buffer);
    }

    <span class="co">// Start stream</span>
    <span class="kw">enum</span> v4l2_buf_type buf_type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    ioctl(fd, VIDIOC_STREAMON, &amp;buf_type);

    <span class="kw">while</span>(<span class="dv">1</span>)
    {
        <span class="co">// Synchronize</span>
        <span class="kw">struct</span> pollfd fds = 
        {
            .fd = fd,
            .events = POLLIN
        };
        poll(&amp;fds, <span class="dv">1</span>, -<span class="dv">1</span>);

        <span class="co">// Dump buffer to stdout</span>
        <span class="kw">struct</span> v4l2_buffer buffer = 
        {
            .type = V4L2_BUF_TYPE_VIDEO_CAPTURE,
            .memory = V4L2_MEMORY_MMAP,
        };
        ioctl(fd, VIDIOC_DQBUF, &amp;buffer);
        write(<span class="dv">1</span>, pbuffers[buffer.index], buffer.bytesused);
        ioctl(fd, VIDIOC_QBUF, &amp;buffer);
    }
}</code></pre></td></tr></table>
<!-- -->

<h1 id="appendixc"><a href="#appendixc">Colorspace conversion example</a></h1>
<p>In this example RGB to YUV color-space conversion is implemented in fragment shader. Each input channel has its own texturing unit, texture coordinates are divided by sub-sampling factor 4:2:0.</p>
<table class="sourceCode c numberLines"><tr class="sourceCode"><td class="lineNumbers"><pre>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="sourceCode"><pre><code class="sourceCode c">uniform sampler2D texY, texU, texV;
varying vec2 texCoord;

<span class="dt">void</span> main()
{
    <span class="dt">float</span> y = texture2D(texY, texCoord).a * <span class="fl">1.1644</span> - <span class="fl">0.062745</span>;
    <span class="dt">float</span> u = texture2D(texU, texCoord / <span class="dv">2</span>).a - <span class="fl">0.5</span>;
    <span class="dt">float</span> v = texture2D(texV, texCoord / <span class="dv">2</span>).a - <span class="fl">0.5</span>;

    gl_FragColor = vec4(
        y + <span class="fl">1.596</span> * v,
        y - <span class="fl">0.39176</span> * v - <span class="fl">0.81297</span> * u,
        y + <span class="fl">2.0172</span> * u,
        <span class="fl">1.0</span>);
}</code></pre></td></tr></table>
<div class="references">
<h1><a href="#references">References</a></h1>
<p>[1] BIMBER, Oliver and RASKAR, Ramesh. <em>Spatial augmented reality: merging real and virtual worlds</em>. Wellesley: A K Peters, 2005. ISBN 15-688-1230-2. </p>
<p>[2] LinuxTV Developers. Linux Media Infrastructure API. [online]. 2012. [Accessed 20 November 2013]. Available from: <a href="http://linuxtv.org/downloads/v4l-dvb-apis" title="http://linuxtv.org/downloads/v4l-dvb-apis">http://linuxtv.org/downloads/v4l-dvb-apis</a></p>
<p>[3] Consultative Committee on International Radio. Recommendation BT.601. [online]. 2011. [Accessed 20 November 2013]. Available from: <a href="http://www.itu.int/rec/R-REC-BT.601/en" title="http://www.itu.int/rec/R-REC-BT.601/en">http://www.itu.int/rec/R-REC-BT.601/en</a></p>
<p>[4] TI OMAP Developers. TI OMAP Trunk PPA. [online]. 2013. [Accessed 20 November 2013]. Available from: <a href="https://launchpad.net/~tiomap-dev/+archive/omap-trunk" title="https://launchpad.net/~tiomap-dev/+archive/omap-trunk">https://launchpad.net/~tiomap-dev/+archive/omap-trunk</a></p>
<p>[5] Khronos Group. OpenGL ES 2.x - for Programmable Hardware. [online]. 2013. [Accessed 20 November 2013]. Available from: <a href="http://www.khronos.org/opengles/2_X" title="http://www.khronos.org/opengles/2_X">http://www.khronos.org/opengles/2_X</a></p>
<p>[6] JAZAR, Reza N. <em>Theory of applied robotics: kinematics, dynamics, and control</em>. 2nd ed. New York: Springer, 2010. ISBN 978-1-4419-1749-2. </p>
<p>[7] KENNEDY, Melita. <em>Understanding map projections</em>. Redlands: ESRI, 2000. ISBN 15-894-8003-1. </p>
<p>[8] National Marine Electronics Association. NMEA 0183. [online]. 2008. [Accessed 20 November 2013]. Available from: <a href="http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp" title="http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp">http://www.nmea.org/content/nmea_standards/nmea_0183_v_410.asp</a></p>
<p>[9] Texas Instruments. OMAP 4460 Multimedia Device. [online]. 2012. [Accessed 20 November 2013]. Available from: <a href="http://www.ti.com/product/omap4460" title="http://www.ti.com/product/omap4460">http://www.ti.com/product/omap4460</a></p>
<p>[10] InvenSense. MPU-9150 Nine-axis MEMS MotionTracking™ Device. [online]. 2013. [Accessed 20 November 2013]. Available from: <a href="http://www.invensense.com/mems/gyro/mpu9150.html" title="http://www.invensense.com/mems/gyro/mpu9150.html">http://www.invensense.com/mems/gyro/mpu9150.html</a></p>
</div>
</body>
</html>
